{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESTIMATED TOTAL MEMORY USAGE: 2700 MB (but peaks will hit ~20 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals of this notebook\n",
    "\n",
    "We want to introduce the basics of neural networks and deep learning. Modern deep learning is a huge field and it's impossible to cover even all the significant developments in the last 5 years here. But the basics are straightforward.\n",
    "\n",
    "One big caveat: deep learning is a rapidly evolving field. There are new developments in neural network architectures, novel applications, better optimization techniques, theoretical results justifying why something works etc. daily. It's a great opportunity to get involved if you find research interesting and there are great online communities (pytorch, fast.ai, paperswithcode, pysyft) that you should get involved with.\n",
    "\n",
    "**Note**: Unlike the previous notebooks, this notebook has very few questions. You should study the code, tweak the data, the parameters, and poke the models to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: You can install extensions (google for nbextensions) with Jupyter notebooks. I tend to use resuse to display memory usage in the top right corner which really helps.\n",
    "\n",
    "To run a cell, press: \"Shift + Enter\"\n",
    "\n",
    "To add a cell before your current cell, press: \"Esc + a\"\n",
    "\n",
    "To add a cell after your current cell, press: \"Esc + b\"\n",
    "\n",
    "To delete a cell, press: \"Esc + x\"\n",
    "\n",
    "To be able to edit a cell, press: \"Enter\"\n",
    "\n",
    "To see more documentation about of a function, type ?function_name\n",
    "\n",
    "To see source code, type ??function_name\n",
    "\n",
    "To quickly see possible arguments for a function, type \"Shift + Tab\" after typing the function name.\n",
    "\n",
    "Esc and Enter take you into different modes. Press \"Esc + h\" to see all shortcuts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic/Artificial Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered the basics of neural networks in the lecture. We also saw applications to two synthetic datasets. The goal in this section is to replicate those results and get a feel for using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_data(N_examples=1000, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    features = []\n",
    "    target = []\n",
    "\n",
    "    for i in range(N_examples):\n",
    "        #class = 0\n",
    "        r = np.random.uniform()\n",
    "        theta = np.random.uniform(0, 2*np.pi)\n",
    "\n",
    "        features.append([r*np.cos(theta), r*np.sin(theta)])\n",
    "        target.append(0)\n",
    "\n",
    "        #class = 1\n",
    "        r = 3 + np.random.uniform()\n",
    "        theta = np.random.uniform(0, 2*np.pi)\n",
    "\n",
    "        features.append([r*np.cos(theta), r*np.sin(theta)])\n",
    "        target.append(1)\n",
    "\n",
    "    features = np.array(features)\n",
    "    target = np.array(target)\n",
    "\n",
    "    return features, target    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = generate_binary_data(seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_data(features, target):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(features[target==0][:,0], features[target==0][:,1], 'p', color='r', label='0')\n",
    "    plt.plot(features[target==1][:,0], features[target==1][:,1], 'p', color='g', label='1')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_data(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two features here - x and y. There is a binary target variable that we need to predict. This is essentially the dataset from the logistic regression discussion. Logistic regression will not do well here given that the data is not linearly separable. Transforming the data so we have two features:\n",
    "\n",
    "$$r^2  = x^2 + y^2$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\theta = \\arctan(\\frac{y}{x})$$\n",
    "\n",
    "would make it very easy to use logistic regression (or just a cut at $r = 2$) to separate the two classes but while it is easy for us to visualize the data and guess at the transformation, in high dimensions, we can't follow the same process.\n",
    "\n",
    "Let's implement a feed-forward neural network that takes the two features as input and predicts the probabiliy of being in class 1 as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNet(nn.Module): #inherit from nn.Module to define your own architecture\n",
    "    def __init__(self, N_inputs, N_outputs, N_hidden_layers, N_hidden_nodes, activation, output_activation):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "        \n",
    "        self.N_inputs = N_inputs #2 in our case\n",
    "        self.N_outputs = N_outputs #1 in our case but can be higher for multi-class classification\n",
    "        \n",
    "        self.N_hidden_layers = N_hidden_layers #we'll start by using one hidden layer\n",
    "        self.N_hidden_nodes = N_hidden_nodes #number of nodes in each hidden layer - can extend to passing a list\n",
    "        \n",
    "        #Define layers below - pytorch has a lot of layers pre-defined\n",
    "        \n",
    "        #use nn.ModuleList or nn.DictList instead of [] or {} - more explanations below\n",
    "        self.layer_list = nn.ModuleList([]) #use just as a python list\n",
    "        for n in range(N_hidden_layers):\n",
    "            if n==0:\n",
    "                self.layer_list.append(nn.Linear(N_inputs, N_hidden_nodes))\n",
    "            else:\n",
    "                self.layer_list.append(nn.Linear(N_hidden_nodes, N_hidden_nodes))\n",
    "        \n",
    "        self.output_layer = nn.Linear(N_hidden_nodes, N_outputs)\n",
    "        \n",
    "        self.activation = activation #activations at inner nodes\n",
    "        self.output_activation = output_activation #activation at last layer (depends on your problem)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        '''\n",
    "        every neural net in pytorch has its own forward function\n",
    "        this function defines how data flows through the architecture from input to output i.e. the forward propagation part\n",
    "        '''\n",
    "        \n",
    "        out = inp\n",
    "        for layer in self.layer_list:\n",
    "            out = layer(out) #calls forward function for each layer (already implemented for us)\n",
    "            out = self.activation(out) #non-linear activation\n",
    "            \n",
    "        #pass activations through last/output layer\n",
    "        out = self.output_layer(out)\n",
    "        if self.output_activation is not None:\n",
    "            pred = self.output_activation(out)\n",
    "        else:\n",
    "            pred = out\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of specifying a neural net architecture in pytorch. You can work at a high level of abstraction by just listing the layers that you want to getting into the fine details by constructing your own layers (as classes) that can be used in ClassifierNet above.\n",
    "\n",
    "How does pytorch work? When you define an architecture like the one above, pytorch constructs a graph (nodes and edges) where the nodes are operations on multi-indexed arrays (called tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 2\n",
    "N_outputs = 1\n",
    "\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 2\n",
    "\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = nn.Sigmoid() #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "We first need to pick our loss function. Like we binary classification problems (including logistic regression), we'll use binary cross-entropy:\n",
    "\n",
    "$$\\text{Loss, } L = -\\Sigma_{i=1}^{N} y_i \\log(p_i) + (1-y_i) \\log(1-p_i)$$\n",
    "\n",
    "where $y_i \\in {0,1}$ are the labels and $p_i \\in [0,1]$ are the probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at all available losses (you can always write your own)\n",
    "torch.nn.*Loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a feel for the loss function\n",
    "#target = 1 (label = 1)\n",
    "print(criterion(torch.tensor(1e-2), torch.tensor(1.))) #pred prob = 1e-2 -> BAD\n",
    "print(criterion(torch.tensor(0.3), torch.tensor(1.))) #pred prob = 0.3 -> BAd\n",
    "print(criterion(torch.tensor(0.5), torch.tensor(1.))) #pred prob = 0.5 -> Bad\n",
    "print(criterion(torch.tensor(1.), torch.tensor(1.))) #pred prob = 1.0 -> GREAT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**:\n",
    "\n",
    "So we have the data, the neural net architecture, a loss function to measure how well the model does on our task. We also need a way to do gradient descent.\n",
    "\n",
    "Recall, we use gradient descent to minimize the loss by computing the first derivative (gradients) and taking a step in the direction opposite (since we are minimizing) to the gradient:\n",
    "\n",
    "$$w_{t} \\rightarrow w_{t} - \\eta \\frac{\\partial L}{\\partial w_{t-1}}$$\n",
    "\n",
    "where $w_t$ = weight at time-step t, $L$ = loss, $\\eta$ = learning rate.\n",
    "\n",
    "For our neural network, we first need to calculate the gradients. Thankfully, this is done automatically by pytorch using a procedure called **backpropagation**. If you are interested in more calculations details, please check \"automatic differentiation\" and an analytical calculation for a feed-forward network (https://treeinrandomforest.github.io/deep-learning/2018/10/30/backpropagation.html).\n",
    "\n",
    "The gradients are calculated by calling a function **backward** on the network, as we'll see below.\n",
    "\n",
    "Once the gradients are calculated, we need to update the weights. In practice, there are many heuristics/variants of the update step above that lead to better optimization behavior. A great resource to dive into details is https://ruder.io/optimizing-gradient-descent/. We won't get into the details here.\n",
    "\n",
    "We'll choose what's called the **Adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked a constant learning rate here (which is adjusted internally by Adam) and also passed all the tunable weights in the network by using: net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 free parameters:\n",
    "\n",
    "* A 2x2 matrix (4 parameters) mapping the input layer to the 1 hidden layer.\n",
    "\n",
    "* A 2x1 matrix (2 parameters) mapping the hidden layer to the output layer with one node.\n",
    "\n",
    "* 2 biases for the 2 nodes in the hidden layer.\n",
    "\n",
    "* 1 bias for the output node in the output layer.\n",
    "\n",
    "\n",
    "This is a good place to explain why we need to use nn.ModuleList. If we had just used a vanilla python list, net.parameters() would only show weights that are explicitly defined in our net architecture. The weights and biases associated with the layers would NOT show up in net.parameters(). This process of a module higher up in the hierarchy (ClassifierNet) subsuming the weights and biases of modules lower in the hierarchy (layers) is called **registering**. ModuleList ensures that all the weights/biases are registered as weights and biases of ClassifierNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all these elements and train our first neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert features and target to torch tensors\n",
    "features = torch.from_numpy(features)\n",
    "target = torch.from_numpy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if have gpu, throw the model, features and labels on it\n",
    "net = net.to(device)\n",
    "features = features.to(device).float()\n",
    "target = target.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the following steps now:\n",
    "\n",
    "* Compute the gradients for our dataset.\n",
    "\n",
    "* Do gradient descent and update the weights.\n",
    "\n",
    "* Repeat till ??\n",
    "\n",
    "The problem is there's no way of knowing when we have converged or are close to the minimum of the loss function. In practice, this means we keep repeating the process above and monitor the loss as well as performance on a hold-out set. When we start over-fitting on the training set, we stop. There are various modifications to this procedure but this is the essence of what we are doing.\n",
    "\n",
    "Each pass through the whole dataset is called an **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 100\n",
    "for epoch in range(N_epochs):\n",
    "    out = net(features) #make predictions on the inputs\n",
    "    loss = criterion(out, target) #compute loss on our predictions\n",
    "    \n",
    "    optimizer.zero_grad() #set all gradients to 0\n",
    "    loss.backward() #backprop to compute gradients\n",
    "    optimizer.step() #update the weights\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combined all these elements into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, target, model, lr, N_epochs, criterion=nn.BCELoss(), shuffle=False):\n",
    "    #criterion = nn.BCELoss() #binary cross-entropy loss as before\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #Adam optimizer\n",
    "\n",
    "    #if have gpu, throw the model, features and labels on it\n",
    "    model = model.to(device)\n",
    "    features = features.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    for epoch in range(N_epochs):\n",
    "        if shuffle: #should have no effect on gradients in this case\n",
    "            indices = torch.randperm(len(features))\n",
    "\n",
    "            features_shuffled = features[indices]\n",
    "            target_shuffled = target[indices]\n",
    "        else:\n",
    "            features_shuffled = features\n",
    "            target_shuffled = target\n",
    "\n",
    "        out = model(features_shuffled)\n",
    "        #out = out.reshape(out.size(0))\n",
    "        loss = criterion(out, target_shuffled)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'epoch = {epoch} loss = {loss}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    pred = model(features_shuffled).reshape(len(target))\n",
    "    pred[pred>0.5] = 1\n",
    "    pred[pred<=0.5] = 0\n",
    "\n",
    "    #print(f'Accuracy = {accuracy}')\n",
    "        \n",
    "    model = model.to('cpu')\n",
    "    features = features.to('cpu')\n",
    "    target = target.to('cpu')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Train the model and vary the number of hidden nodes and see what happens to the loss. Can you explain this behavior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 2\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 1 #<--- play with this\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = nn.Sigmoid() #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "net = train_model(features, target, net, 1e-3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 2\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 2 #<--- play with this\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = nn.Sigmoid() #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "net = train_model(features, target, net, 1e-3, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 2\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 3 #<--- play with this\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = nn.Sigmoid() #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "net = train_model(features, target, net, 1e-3, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some \"magic\" behavior when we increase the number of nodes in the first (and only) hidden layer from 2 to 3. Loss suddenly goes down dramatically. At this stage, we should explore why that's happening.\n",
    "\n",
    "For every node in the hidden layer, we have a mapping from the input to that node:\n",
    "\n",
    "$$\\sigma(w_1 x + w_2 y + b)$$\n",
    "\n",
    "where $w_1, w_2, b$ are specific to that hidden node. We can plot the decision line in this case:\n",
    "\n",
    "$$w_1 x + w_2 y + b = 0$$\n",
    "\n",
    "Unlike logistic regression, this is not actually a decision line. Points on one side are not classified as 0 and points on the other side as 1 (if the threshold = 0.5). Instead this line should be thought of as one defining a new coordinate-system. Instead of x and y coordinates, every hidden node induces a straight line and a new coordinate, say $\\alpha_i$. So if we have 3 hidden nodes, we are mapping the 2-dimensional input space into a 3-dimensional space where the coordinates $\\alpha_1, \\alpha_2, \\alpha_3$ for each point depend on which side of the 3 lines induced as mentioned above, it lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params[0]) #3x2 matrix\n",
    "print(params[1]) #3 biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.detach().cpu().numpy() #detach from pytorch computational graph, bring back to cpu, convert to numpy\n",
    "target = target.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "#plot raw data\n",
    "ax.plot(features[target==0][:,0], features[target==0][:,1], 'p', color='r', label='0')\n",
    "ax.plot(features[target==1][:,0], features[target==1][:,1], 'p', color='g', label='1')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "#get weights and biases\n",
    "weights = params[0].detach().numpy()\n",
    "biases = params[1].detach().numpy()\n",
    "\n",
    "#plot straight lines\n",
    "x_min, x_max = features[:,0].min(), features[:,0].max()\n",
    "y_lim_min, y_lim_max = features[:,1].min(), features[:,1].max()\n",
    "for i in range(weights.shape[0]): #loop over each hidden node in the one hidden layer\n",
    "    coef = weights[i]\n",
    "    intercept = biases[i]\n",
    "    \n",
    "    y_min = (-intercept - coef[0]*x_min)/coef[1]\n",
    "    y_max = (-intercept - coef[0]*x_max)/coef[1]\n",
    "    \n",
    "    ax.plot([x_min, x_max], [y_min, y_max])\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_lim_min, y_lim_max)\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot we showed in the lecture. For every hidden node in the hidden layer, we have a straight line. The colors of the three lines above are orange, green and blue and that's what we'll call our new coordinates.\n",
    "\n",
    "Suppose you pick a point in the red region:\n",
    "\n",
    "* It lies to the *right* of the orange line\n",
    "\n",
    "* It lies to the *bottom* of the green line\n",
    "\n",
    "* It lies to the *top* of the blue line.\n",
    "\n",
    "(These directions might change because of inherent randomness during training - weight initializations here).\n",
    "\n",
    "On the other hand, we have **6** green regions. If you start walking clockwise from the top green section, every time you cross a straight line, you walk into a new region. Each time you walk into a new region, you flip the coordinate of one of the 3 lines. Either you go from *right* to *left* of the orange line, *bottom* to *top* of the green line or *top* to *bottom* of the blue line.\n",
    "\n",
    "So instead of describing each point by two coordinates (x, y), we can describe it by (orange status, green status, blue status). We happen to have 7 such regions here - with 1 being purely occupied by the red points and the other 7 by green points.\n",
    "\n",
    "This might be become cleared from a 3-dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get hidden layer activations for all inputs\n",
    "features_layer1_3d = net.activation(net.layer_list[0](torch.tensor(features))).detach().numpy()\n",
    "print(features_layer1_3d[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.plot(features_layer1_3d[target==0][:,0], features_layer1_3d[target==0][:,1], features_layer1_3d[target==0][:,2], 'p', color ='r', label='0')\n",
    "ax.plot(features_layer1_3d[target==1][:,0], features_layer1_3d[target==1][:,1], features_layer1_3d[target==1][:,2], 'p', color ='g', label='1')\n",
    "\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, a simple linear classifier can draw a linear decision boundary (a plane) to separate the red points from the green points. Also, these points lie in the unit cube (cube with sides of length=1) since we are using sigmoid activations. Whenever the activations get saturated (close to 0 or 1), then we see points on the edges and corners of the cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Switch the activation from sigmoid to relu (nn.ReLU()). Does the loss still essentially become zero on the train set? If not, try increasing N_hidden_nodes. At what point does the loss actually become close to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 2\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 5 #<---- play with this\n",
    "activation = nn.ReLU()\n",
    "output_activation = nn.Sigmoid() #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "features = torch.tensor(features)\n",
    "target = torch.tensor(target)\n",
    "\n",
    "net = train_model(features, target, net, 1e-3, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Remake the 3d plot but by trying 3 coordinates out of the N_hidden_nodes coordinates you found above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.detach().cpu().numpy() #detach from pytorch computational graph, bring back to cpu, convert to numpy\n",
    "target = target.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get hidden layer activations for all inputs\n",
    "features_layer1_3d = net.activation(net.layer_list[0](torch.tensor(features))).detach().numpy()\n",
    "\n",
    "print(features_layer1_3d[0:10])\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "COORD1 = 0\n",
    "COORD2 = 1\n",
    "COORD3 = 2\n",
    "\n",
    "ax.plot(features_layer1_3d[target==0][:,COORD1], features_layer1_3d[target==0][:,COORD2], features_layer1_3d[target==0][:,COORD3], 'p', color ='r', label='0')\n",
    "ax.plot(features_layer1_3d[target==1][:,COORD1], features_layer1_3d[target==1][:,COORD2], features_layer1_3d[target==1][:,COORD3], 'p', color ='g', label='1')\n",
    "\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "COORD1 = 0\n",
    "COORD2 = 1\n",
    "COORD3 = 3\n",
    "\n",
    "ax.plot(features_layer1_3d[target==0][:,COORD1], features_layer1_3d[target==0][:,COORD2], features_layer1_3d[target==0][:,COORD3], 'p', color ='r', label='0')\n",
    "ax.plot(features_layer1_3d[target==1][:,COORD1], features_layer1_3d[target==1][:,COORD2], features_layer1_3d[target==1][:,COORD3], 'p', color ='g', label='1')\n",
    "\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "COORD1 = 0\n",
    "COORD2 = 2\n",
    "COORD3 = 3\n",
    "\n",
    "ax.plot(features_layer1_3d[target==0][:,COORD1], features_layer1_3d[target==0][:,COORD2], features_layer1_3d[target==0][:,COORD3], 'p', color ='r', label='0')\n",
    "ax.plot(features_layer1_3d[target==1][:,COORD1], features_layer1_3d[target==1][:,COORD2], features_layer1_3d[target==1][:,COORD3], 'p', color ='g', label='1')\n",
    "\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "COORD1 = 1\n",
    "COORD2 = 2\n",
    "COORD3 = 3\n",
    "\n",
    "ax.plot(features_layer1_3d[target==0][:,COORD1], features_layer1_3d[target==0][:,COORD2], features_layer1_3d[target==0][:,COORD3], 'p', color ='r', label='0')\n",
    "ax.plot(features_layer1_3d[target==1][:,COORD1], features_layer1_3d[target==1][:,COORD2], features_layer1_3d[target==1][:,COORD3], 'p', color ='g', label='1')\n",
    "\n",
    "ax.legend(framealpha=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw all the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "for comb in itertools.combinations(np.arange(N_hidden_nodes), 3):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    COORD1 = comb[0]\n",
    "    COORD2 = comb[1]\n",
    "    COORD3 = comb[2]\n",
    "\n",
    "    ax.plot(features_layer1_3d[target==0][:,COORD1], features_layer1_3d[target==0][:,COORD2], features_layer1_3d[target==0][:,COORD3], 'p', color ='r', label='0')\n",
    "    ax.plot(features_layer1_3d[target==1][:,COORD1], features_layer1_3d[target==1][:,COORD2], features_layer1_3d[target==1][:,COORD3], 'p', color ='g', label='1')\n",
    "\n",
    "    ax.legend(framealpha=0)\n",
    "    \n",
    "    plt.title(f'COORDINATES = {comb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Generally it is a good idea to use a linear layer for the output layer and use BCEWithLogitsLoss to avoid numerical instabilities. We will do this later for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = None\n",
    "features_layer1_3d = None\n",
    "target = None\n",
    "net = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(L=10, stepsize=0.1):\n",
    "    x = np.arange(-L, L, stepsize)\n",
    "    y = np.sin(3*x) * np.exp(-x / 8.)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def plot_regression_data(x, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_regression_data()\n",
    "plot_regression_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty different problem in some ways. We now have one input - x and one output - y. But looked at another way, we simply change the number of inputs in our neural network to 1 and we change the output activation to be a linear function. Why linear? Because in principle, the output (y) can be unbounded i.e. any real value.\n",
    "\n",
    "We also need to change the loss function. While binary cross-entropy is appropriate for a classification problem, we need something else for a regression problem. We'll use mean-squared error:\n",
    "\n",
    "$$\\frac{1}{2}(y_{\\text{target}} - y_{\\text{pred}})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modifying N_hidden_nodes from 1 through 10 and see what happens to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 1\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10 #<--- play with this\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = None #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "features = torch.tensor(x).float().reshape(len(x), 1)\n",
    "target = torch.tensor(y).float().reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train_model(features, target, net, 1e-2, 20000, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net(features).cpu().detach().numpy().reshape(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to understand what the model is doing. As before, let's consider the mapping from the input node to one node of the hidden layer. In this case, we have the mapping:\n",
    "\n",
    "$$\\sigma(w_i x + b_i)$$\n",
    "\n",
    "where $w_i, b_i$ are the weight and bias associated with each node of the hidden layer. This defines a \"decision\" boundary where:\n",
    "\n",
    "$$w_i x + b_i = 0$$\n",
    "\n",
    "This is just a value $\\delta_{i} \\equiv -\\frac{b_i}{w_i}$. \n",
    "\n",
    "For each hidden node $i$, we can calculate one such threshold, $\\delta_i$.\n",
    "\n",
    "As we walk along the x-axis from the left to right, we will cross each threshold one by one. On crossing each threshold, one hidden node switches i.e. goes from $0 \\rightarrow 1$ or $1 \\rightarrow 0$. What effect does this have on the output or prediction?\n",
    "\n",
    "Since the last layer is linear, its output is:\n",
    "\n",
    "$y = v_1 h_1 + v_2 h_2 + \\ldots + v_n h_n + c$\n",
    "\n",
    "where $v_i$ are the weights from the hidden layer to the output node, $c$ is the bias on the output node, and $h_i$ are the activations on the hidden nodes. These activations can smoothly vary between 0 and 1 according to the sigmoid function.\n",
    "\n",
    "So, when we cross a threshold, one of the $h_j$ values eithers turns off or turns on. This has the effect of adding or subtracting constant $v_k$ values from the output if the kth hidden node, $h_k$ is switching on/off.\n",
    "\n",
    "This means that as we add more hidden nodes, we can divide the domain (the x values) into more fine-grained intervals that can be assigned a single value by the neural network. In practice, there is a smooth interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Suppose instead of the sigmoid activations, we used a binary threshold:\n",
    "\n",
    "$$\\sigma(x) = \\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "0 & x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "then we would get a piece-wise constant prediction from our trained network. Plot that piecewise function as a function of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = net.activation(net.layer_list[0](features))\n",
    "print(activations[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_activations = nn.Threshold(0.5, 0)(activations)/activations\n",
    "print(binary_activations[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_pred = net.output_layer(binary_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y, label='data')\n",
    "plt.plot(x, binary_pred.cpu().detach().numpy(), label='binary')\n",
    "plt.plot(x, pred, color='r', label='pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why does the left part of the function fit so well but the right side is always compromised? Hint: think of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely reason is that the loss function is sensitive to the scale of the $y$ values. A 10% deviation between the y-value and the prediction near x = -10 has a larger absolute value than a 10% deviation near say, x = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Can you think of ways to test this hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things you could do. One is to flip the function from left to right and re-train the model. In this case, the right side should start fitting better.\n",
    "\n",
    "Another option is to change the loss function to percentage error i.e.:\n",
    "\n",
    "$$\\frac{1}{2} \\big(\\frac{y_{\\text{target}} - y_{\\text{pred}}}{y_{\\text{target}}}\\big)^2$$\n",
    "\n",
    "but this is probably much harder to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = copy.copy(y[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inputs = 1\n",
    "N_outputs = 1\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.Sigmoid()\n",
    "output_activation = None #we want one probability between 0-1\n",
    "\n",
    "net = ClassifierNet(N_inputs,\n",
    "                    N_outputs,\n",
    "                    N_hidden_layers,\n",
    "                    N_hidden_nodes,\n",
    "                    activation,\n",
    "                    output_activation)\n",
    "\n",
    "features = torch.tensor(x).float().reshape(len(x), 1)\n",
    "target = torch.tensor(y).float().reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train_model(features, target, net, 1e-2, 14000, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net(features).cpu().detach().numpy().reshape(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, now the right side of the function fits well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = net.activation(net.layer_list[0](features))\n",
    "binary_activations = nn.Threshold(0.5, 0)(activations)/activations\n",
    "binary_pred = net.output_layer(binary_activations)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y, label='data')\n",
    "plt.plot(x, binary_pred.cpu().detach().numpy(), label='binary')\n",
    "plt.plot(x, pred, color='r', label='pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you should restart the kernel and clear the output since we don't need anything from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most successful applications of deep learning has been to computer vision. A central task of computer vision is **image classification**. This is the task of assigning exactly one of multiple labels to an image. \n",
    "\n",
    "pytorch provides a package called **torchvision** which includes datasets, some modern neural network architectures as well as helper functions for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_PATH = \"../data/MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(DOWNLOAD_PATH, \n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "mnist_test = MNIST(DOWNLOAD_PATH, \n",
    "                   train=False, \n",
    "                   download=True,\n",
    "                   transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely run into memory issues between the data and the weights/biases of your neural network. Let's instead sample 1/10th the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "[35098 55480 27386  2449 52502 38110 14278 53009 44689 20252]\n",
      "torch.Size([6000, 28, 28])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train.data.shape)\n",
    "print(mnist_train.targets.shape)\n",
    "\n",
    "N_choose = 6000\n",
    "\n",
    "chosen_ids = np.random.choice(np.arange(mnist_train.data.shape[0]), N_choose)\n",
    "print(chosen_ids[0:10])\n",
    "\n",
    "print(mnist_train.data[chosen_ids, :, :].shape)\n",
    "print(mnist_train.targets[chosen_ids].shape)\n",
    "\n",
    "mnist_train.data = mnist_train.data[chosen_ids, :, :]\n",
    "mnist_train.targets = mnist_train.targets[chosen_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n",
      "[7142 2853 3513 9725  936 7518 8208 2364 7900   74]\n",
      "torch.Size([1000, 28, 28])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_test.data.shape)\n",
    "print(mnist_test.targets.shape)\n",
    "\n",
    "N_choose = 1000\n",
    "\n",
    "chosen_ids = np.random.choice(np.arange(mnist_test.data.shape[0]), N_choose)\n",
    "print(chosen_ids[0:10])\n",
    "\n",
    "print(mnist_test.data[chosen_ids, :, :].shape)\n",
    "print(mnist_test.targets[chosen_ids].shape)\n",
    "\n",
    "mnist_test.data = mnist_test.data[chosen_ids, :, :]\n",
    "mnist_test.targets = mnist_test.targets[chosen_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is one of the classic image datasets and consists of 28 x 28 pixel images of handwritten digits. We downloaded both the train and test sets. Transforms defined under target_transform will be applied to each example. In this example, we want tensors and not images which is what the transforms do.\n",
    "\n",
    "The train set consists of 60000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  37, 234,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  61, 254, 167,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  45, 244, 202,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0, 215, 249,  27,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           2,  76, 227, 184,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26,\n",
       "         212, 254, 254, 184,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 158,\n",
       "         254, 221, 254, 184,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 222, 254,\n",
       "         179,  66, 254, 184,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 104, 228, 254, 178,\n",
       "          12,  61, 254, 108,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  35, 192, 253, 227, 128,   4,\n",
       "           0, 159, 254,  85,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  48,  66, 114, 239, 254, 178,  68,   0,   0,\n",
       "           5, 236, 254,  85,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  35, 238, 254, 254, 254, 254, 186, 160, 124,  61,\n",
       "          65, 254, 254, 125,  61,  61,  61,  61, 128,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  31, 183, 239, 200, 239, 247, 254, 254, 254, 254,\n",
       "         254, 254, 254, 254, 254, 254, 254, 249, 119,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  44,  85,  85, 124, 183,\n",
       "         241, 254, 250, 184, 184, 153,  85,  55,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         117, 254, 151,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         106, 254,  69,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         106, 254, 224,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         180, 254, 239,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         205, 254, 164,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         166, 254,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa06f2d790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 unique labels - 0 through 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 7, 8, 4, 6, 0, 0, 6, 4, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.targets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are roughly equally/uniformly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([610, 664, 574, 575, 584, 547, 591, 629, 630, 596]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_train.targets, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set consists of 10000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa06f22690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist_test.data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 3, 2, 1, 8, 2, 2, 3, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test.targets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([ 98, 123,  92, 114,  88,  98, 100,  97, 103,  87]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_test.targets, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Classifier**:\n",
    "\n",
    "We first have to pick an architecture. The first one we'll pick is a feed-forward neural network like the one we used in the exercises above. This time I am going to use a higher abstraction to define the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 28x28 image -> 784-dimensional flattened vector\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.flatten(start_dim=1, end_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flatten()(mnist_train.data[0:10]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture definition using nn.Sequential. You can just list the layers in a sequence. We carry out the following steps:\n",
    "\n",
    "* Flatten each image into a 784 dimensional vector\n",
    "\n",
    "* Map the image to a 100-dimensional vector using a linear layer\n",
    "\n",
    "* Apply a relu non-linearity\n",
    "\n",
    "* Map the 100-dimensional vector into a 10-dimensional output layer since we have 10 possible targets.\n",
    "\n",
    "* Apply a softmax activation to convert the 10 numbers into a probability distribution that assigns the probability the image belonging to each class (0 through 9)\n",
    "\n",
    "A softmax activation takes N numbers $a_1, \\ldots, a_{10}$ and converts them to a probability distribution. The first step is to ensure the numbers are positive (since probabilities cannot be negative). This is done by exponentiation.\n",
    "\n",
    "$$a_i \\rightarrow e^{a_i}$$\n",
    "\n",
    "The next step is to normalize the numbers i.e. ensure they add up to 1. This is very straightforward. We just divide each score by the sum of scores:\n",
    "\n",
    "$$p_i = \\frac{e^{a_i}}{e^{a_1} + e^{a_2} + \\ldots + e^{a_N}}$$\n",
    "\n",
    "This is the softmax function. If you have done statistical physics (physics of systems with very large number of interacting constituents), you probably have seen the Boltzmann distribution:\n",
    "\n",
    "$$p_i = \\frac{e^{-\\beta E_i}}{e^{-\\beta E_1} + e^{-\\beta E_2} + \\ldots + e^{-\\beta E_N}}$$\n",
    "\n",
    "which gives the probability that a system with N energy levels is in the state with energy $i$ when it is in equilibrium with a thermal bath at temperature $T = \\frac{1}{k_B\\beta}$. This is the only probability distribution that is invariant to constant shifts in energy: $E_i \\rightarrow E_i + \\Delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_net = nn.Sequential(Flatten(), \n",
    "                             nn.Linear(784, 100),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(100, 10),\n",
    "                             nn.Softmax(dim=1) #convert 10-dim activation to probability distribution\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure the data flows through our neural network and check the dimensions. As before, the neural net object is a python callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_net(mnist_train.data[0:12].float()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a 10-dimensional output as expected.\n",
    "\n",
    "**Question**: Check that the outputs for each image are actually a probability distribution (the numbers add up to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_net(mnist_train.data[0:10].float()).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: We have an architecture for our neural network but we now need to decide what loss to pick. Unlike the classification problem earlier which had two classes, we have 10 classes here. Take a look at the pytorch documentation - what loss do you think we should pick to model this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used cross-entropy loss on days 2 and 3. We need the same loss here. Pytorch provides NLLLoss (negative log likelihood) as well as CrossEntropyLoss.\n",
    "\n",
    "**Question**: Look at the documentation for both of these loss functions. Which one should we pick? Do we need to make any modifications to our architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Cross-entropy Loss which can work with the raw scores (without a softmax layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_net = nn.Sequential(Flatten(), \n",
    "                             nn.Linear(784, 100),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(100, 10),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get raw unnormalized scores that were used to compute the probabilities. We should use nn.CrossEntropyLoss in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.2193,  10.0068,  12.8870,  11.3061, -15.1360,  11.1081,   7.4001,\n",
       "         -22.5919,  13.8098, -20.9248],\n",
       "        [ 25.5644,   4.2377,   3.4963,   0.2628, -23.4590,  13.3972, -14.4767,\n",
       "         -15.6164,  -1.9614, -10.9032],\n",
       "        [ 25.3929, -20.3535,   2.0941,  21.1833,  -0.3783,  -8.6609, -14.6873,\n",
       "         -32.4949,  14.1083,   3.5790],\n",
       "        [-20.1825,   4.8184,  -2.7235,   2.5652, -19.2616,   2.1931, -14.8781,\n",
       "         -21.9873,  14.4472,  11.8669],\n",
       "        [  4.6108,  31.8331,  12.2621,  31.0500,   9.2604,  12.6513,   3.9062,\n",
       "         -28.3357,  12.4530,   2.4228],\n",
       "        [  9.9234,  -9.2301,  -0.3042,  33.9055, -11.7333,  11.0156,  16.9658,\n",
       "         -12.6147,  18.5896, -15.0312],\n",
       "        [-25.7316,  -8.4747,  11.5982,   2.9915,   6.0303,  12.8823, -12.5256,\n",
       "         -27.0134,  14.8235,   1.2291],\n",
       "        [ -5.5972,   6.5158,  -4.5050,   1.0942, -25.2070,  20.0081, -16.4611,\n",
       "         -11.6033,  20.9783,  -0.0547],\n",
       "        [ -4.7276,  15.1844,   1.8758,   1.0263,  -5.7444,   0.2223,   2.8069,\n",
       "          -5.3366,  18.4641,   8.0950],\n",
       "        [ 20.8202, -17.8795,   5.2708,   8.9668,  -0.9968,  -3.2737,   1.5939,\n",
       "         -10.7927,   8.5308, -11.0114],\n",
       "        [ 13.3702,  17.5154,  16.3056,  10.1049,   1.0798,  28.1982, -10.8766,\n",
       "         -45.0358,  14.0774, -23.7426],\n",
       "        [ 11.3780,  -2.5742,  22.7985,  37.1000, -14.9697,  -5.8822,  -8.5497,\n",
       "         -22.2688,  20.9844,  12.7354]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_net(mnist_train.data[0:12].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**: We have an architecture, the data, an appropriate loss. Now we need to loop over the images, use the loss to compare the predictions to the targets, compute the gradients and update the weights.\n",
    "\n",
    "In our previous examples, we had N_epoch passes over our dataset and each time, we computed predictions for the full dataset. This is impractical as datasets gets larger. Instead, we need to split the data into **batches** of a fixed size, compute the loss, the gradients and update the weights for each batch.\n",
    "\n",
    "\n",
    "pytorch provides a DataLoader class that makes it easy to generate batches from your dataset.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "Let's analyze how using batches can be different from using the full dataset. Suppose our data has 10,000 rows but we use batches of size 100 (usually we pick powers of 2 for the GPU but this is just an example). Statistically, our goal is always to compute the gradient:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i}$$\n",
    "\n",
    "for all the weights $w_i$. By weights here, I mean both the weights and biases and any other free or tunable parameters in our model.\n",
    "\n",
    "In practice, the loss is a sum over all the examples in our dataset:\n",
    "\n",
    "$$L = \\frac{1}{N}\\Sigma_{i}^N l(p_i, t_i)$$\n",
    "\n",
    "where $p_i$ = prediction for ith example, $t_i$ = target/label for ith example. So the derivative is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = \\frac{1}{N}\\Sigma_i^N \\frac{\\partial l(p_i, t_i)}{\\partial w_i} $$\n",
    "\n",
    "In other words, our goal is to calculate this quantity but $N$ is too large. So we pick a randomly chosen subset of size 100 and only average the gradients over those examples. As an analogy, if our task was to measure the average height of all the people in the world which is impractical, we would pick randomly chosen subsets, say of 10,000 people and measure their average heights. \n",
    "\n",
    "Of course, as we make the subset smaller, the estimate we get will be noisier i.e. it has a greater chance of higher deviation from the actual value (height or gradient). Is this good or bad? It depends. In our case, we are optimizing a function (the loss) that has multiple local minima and saddle points. It is easy to get stuck in regions of the loss space/surface. Having noisy gradients can help with escaping those local minima just because we'll not always be moving in the direction of the true gradient but a noisy estimate.\n",
    "\n",
    "Some commonly used terminology in case you read papers/articles:\n",
    "\n",
    "* (Full) Gradient Descent - compute the gradients over the full dataset. Memory-intensive for larger datasets. This is what we did with our toy examples above.\n",
    "\n",
    "* Mini-batch Gradient Descent - use randomly chosen samples of fixed size as your data. Noisier gradients, more frequent updates to your model, memory efficient.\n",
    "\n",
    "* Stochastic Gradient Descent - Mini-batch gradient descent with batch size = 1. Very noisy estimate, \"online\" updates to your model, can be hard to converge.\n",
    "\n",
    "There are some fascinating papers on more theoretical investigations into the loss surface and the behavior of gradient descent. Here are some examples:\n",
    "\n",
    "* https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf\n",
    "\n",
    "* https://arxiv.org/abs/1811.03804\n",
    "\n",
    "* https://arxiv.org/pdf/1904.06963.pdf\n",
    "\n",
    "\n",
    "**End of optional section**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 #number of examples to compute gradients over (a batch)\n",
    "\n",
    "\n",
    "#python convenience classes to sample and create batches\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, \n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, #shuffle data\n",
    "                                               num_workers=8,\n",
    "                                               pin_memory=True\n",
    "                                             )\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, \n",
    "                                              batch_size=BATCH_SIZE, \n",
    "                                              shuffle=True, #shuffle data\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, (data_example, target_example) = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(idx)\n",
    "print(data_example.shape)\n",
    "print(target_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have batch 0 with 64 tensors of shape (1, 28, 28) and 64 targets. Let's ensure our network can forward propagate on this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [448 x 28], m2: [784 x 100] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ea1eeb248dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_ff_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dt4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dt4/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dt4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dt4/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dt4/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [448 x 28], m2: [784 x 100] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "image_ff_net(data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Debug this error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first shape 1792 x 28 gives us a clue. We want the two 28 sized dimensions to be flattened. But it seems like the wrong dimensions are being flattened here.\n",
    "\n",
    "1792 = 64 * 28\n",
    "\n",
    "We need to rewrite our flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 28x28 image -> 784-dimensional flattened vector\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.flatten(start_dim=1, end_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 784])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flatten()(data_example).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_net = nn.Sequential(Flatten(), \n",
    "                             nn.Linear(784, 100),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(100, 10),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_net(data_example).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the elements together now and write our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 28x28 image -> 784-dimensional flattened vector\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.flatten(start_dim=1, end_dim=-1)\n",
    "    \n",
    "#ARCHITECTURE\n",
    "image_ff_net = nn.Sequential(Flatten(), \n",
    "                             nn.Linear(784, 100),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(100, 10),\n",
    "                            )\n",
    "\n",
    "#LOSS CRITERION and OPTIMIZER\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(image_ff_net.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "#DATALOADERS\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, \n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, #shuffle data\n",
    "                                               num_workers=8,\n",
    "                                               pin_memory=True\n",
    "                                             )\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, \n",
    "                                              batch_size=BATCH_SIZE, \n",
    "                                              shuffle=True, #shuffle data\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 Loss = 0.493075872083505\n",
      "Epoch = 5 Loss = 0.11696399322152137\n",
      "Epoch = 10 Loss = 0.0984588055262963\n",
      "Epoch = 15 Loss = 0.07458434728781382\n"
     ]
    }
   ],
   "source": [
    "image_ff_net.train() #don't worry about this (for this notebook)\n",
    "image_ff_net.to(device)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss_list = []\n",
    "    for idx, (data_example, data_target) in enumerate(train_dataloader):\n",
    "        data_example = data_example.to(device)\n",
    "        data_target = data_target.to(device)\n",
    "        \n",
    "        pred = image_ff_net(data_example)\n",
    "\n",
    "        loss = criterion(pred, data_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "    if epoch % 5 == 0:        \n",
    "        print(f'Epoch = {epoch} Loss = {np.mean(loss_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Use your trained network to compute the accuracy on both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_net = image_ff_net.eval() #don't worry about this (for this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use argmax to extract the label with the highest probability (or the least negative raw score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 7, 4, 4, 6, 4, 3, 4, 6, 1, 5, 1, 7, 0], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_net(data_example).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred, train_targets = torch.tensor([]), torch.tensor([])\n",
    "with torch.no_grad(): #context manager for inference since we don't need the memory footprint of gradients\n",
    "    for idx, (data_example, data_target) in enumerate(train_dataloader):\n",
    "        data_example = data_example.to(device)\n",
    "        \n",
    "        #make predictions\n",
    "        label_pred = image_ff_net(data_example).argmax(dim=1).float()\n",
    "        \n",
    "        #concat and store both predictions and targets\n",
    "        label_pred = label_pred.to('cpu')\n",
    "        train_pred = torch.cat((train_pred, label_pred))\n",
    "        train_targets = torch.cat((train_targets, data_target.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 9., 9., 2., 9., 3., 0., 1., 0., 2.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 9., 9., 2., 9., 3., 0., 1., 0., 2.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(train_pred == train_targets).item() / train_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.9930\n"
     ]
    }
   ],
   "source": [
    "assert(train_pred.shape == train_targets.shape)\n",
    "train_accuracy = torch.sum(train_pred == train_targets).item() / train_pred.shape[0]\n",
    "print(f'Train Accuracy = {train_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I want to make an elementary remark about significant figures. While interpreting numbers like accuracy, it is important to realize how big your dataset and what impact flipping one example from a wrong prediction to the right prediction would have.\n",
    "\n",
    "In our case, the train set has 60,000 examples. Suppose we were to flip one of the incorrectly predicted examples to a correct one (by changing the model, retraining etc etc.). This should change our accuracy, all other examples being the same, by \n",
    "\n",
    "$$\\frac{1}{60,000} = 1.66 * 10^{-5}$$\n",
    "\n",
    "Any digits in the accuracy beyond the fifth place have no meaning! For our test set, we have 10,000 examples so we should only care at most about the 4th decimal place (10,000 being a \"nice\" number i.e. a power of 10 will ensure we never have more any way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred, test_targets = torch.tensor([]), torch.tensor([])\n",
    "with torch.no_grad(): #context manager for inference since we don't need the memory footprint of gradients\n",
    "    for idx, (data_example, data_target) in enumerate(test_dataloader):\n",
    "        data_example = data_example.to(device)\n",
    "        \n",
    "        #make predictions\n",
    "        label_pred = image_ff_net(data_example).argmax(dim=1).float()\n",
    "        \n",
    "        #concat and store both predictions and targets\n",
    "        label_pred = label_pred.to('cpu')\n",
    "        test_pred = torch.cat((test_pred, label_pred))\n",
    "        test_targets = torch.cat((test_targets, data_target.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Accuracy = 0.9440\n"
     ]
    }
   ],
   "source": [
    "assert(test_pred.shape == test_targets.shape)\n",
    "test_accuracy = torch.sum(test_pred == test_targets).item() / test_pred.shape[0]\n",
    "print(f'Test  Accuracy = {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! so our simple neural network already does a great job on our task. At this stage, we would do several things:\n",
    "\n",
    "* Look at the examples being classified incorrectly. Are these bad data examples? Would a person also have trouble classifying them?\n",
    "\n",
    "* Test stability - what happens if we rotate images? Translate them? Flip symmetric digits? What happens if we add some random noise to the pixel values?\n",
    "\n",
    "While we might add these to future iterations of this notebook, let's move on to some other architectural choices. One of the issues with flattening the input image is that of **locality**. Images have a notion of locality. If a pixel contains part of an object, its neighboring pixels are very likely to contain the same object. But when we flatten an image, we use all the pixels to map to each hidden node in the next layer. If we could impose locality by changing our layers, we might get much better performance.\n",
    "\n",
    "In addition, we would like image classification to be invariant to certain transformations like translation (move the digit up/down, left/right), scaling (zoom in and out without cropping the image), rotations (at least upto some angular width). Can we impose any of these by our choice of layers?\n",
    "\n",
    "The answer is yes! Convolutional layers are layers designed specifically to capture such locality and preserve translational invariance. There is a lot of material available describing what these are and we won't repeat it here. Instead, we'll repeat the training procedure above but with convolutional layers.\n",
    "\n",
    "FUTURE TODO: Add analysis of incorrectly predicted examples\n",
    "\n",
    "FUTURE TODO: add a notebook for image filters, convolutions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a convolutional layer:\n",
    "\n",
    "nn.Conv2d\n",
    "\n",
    "which takes in the number of input channels (grayscale), number of output channels (we'll choose 20), kernel size (3x3) and run the transformations on some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, (data_example, target_example) = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16, 20, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "print(data_example.shape)\n",
    "print(nn.Conv2d(1, 20, 3)(data_example).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: If you do know what convolutions are and how filters work, justify these shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension is the batch size which remains unchanged, as expected. In the raw data, the second dimension is the number of channels i.e. grayscale only and the last two dimensions are the size of the image - 28x28.\n",
    "\n",
    "We choose 20 channels which explains the output's second dimension. Each filter is 3x3 and since we have no padding, it can only process 26 patches in each dimension.\n",
    "\n",
    "If we label the pixels along the columns as 1, 2, ..., 28, the patch can be applied from pixels 1-3 (inclusive of both end-points), 2-5, ..., 26-28. After that, the patch \"falls off\" the image unless we apply some padding. This explains the dimension 26 in both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply a ReLU activation to all these activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 26, 26])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.ReLU()((nn.Conv2d(1, 20, 3)(data_example)))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also apply some kind of pooling or averaging now. This reduces noise by picking disjoint, consecutive patches on the image and replacing them by some aggregate statistic like max or mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 13, 13])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.MaxPool2d(kernel_size=2)(nn.ReLU()((nn.Conv2d(1, 20, 3)(data_example))))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A couple of notes**:\n",
    "\n",
    "* Pytorch's functions like nn.ReLU() and nn.MaxPool2d() return functions that can apply operations. So, nn.MaxPool2d(kernel_size=2) returns a function that is then applied to the argument above.\n",
    "\n",
    "* Chaining together the layers and activations and testing them out like above is very valuable as the first step in ensuring your network does what you want it to do.\n",
    "\n",
    "In general, we would suggest the following steps when you are expressing a new network architecture:\n",
    "\n",
    "* Build up your network using nn.Sequential if you are just assembling existing or user-defined layers, or by defining a new network class inheriting from nn.Module where you can define a custom forward function.\n",
    "\n",
    "* Pick a small tensor containing your features and pass it through each step/layer. Ensure the dimensions of the input and output tensors to each layer make sense.\n",
    "\n",
    "* Pick your loss and optimizer and train on a small batch. You should be able to overfit i.e. get almost zero loss on this small set. Neural networks are extremely flexible learners and if you can't overfit on a small batch, you either have a bug or need to add some more capacity (more nodes, more layers etc. -> more weights).\n",
    "\n",
    "* Now you should train on the full train set and practice the usual cross-validation practices.\n",
    "\n",
    "* Probe your model: add noise to the inputs, see where the model isn't performing well, make partial dependency plots etc. to understand characteristics of your model. This part can be very open-ended and it depends on what your final aim is. If you are building a model to predict the stock price so you can trade, you'll spend a lot of time in this step. If you are having fun predicting dogs vs cats, maybe you don't care so much. If your aim is to dive deeper into deep learning, looking at the weights, activations, effect of changing hyperparameters, removing edges/weights etc. are very valuable experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have seen one iteration of applying a convolutional layer followed by a non-linearity and then a max pooling layer. We can add more and more of these elements. As you can see, at each step, we are increasing the number of channels increase but the size of the images decreases because of the convolutions and max pooling.\n",
    "\n",
    "**Question**: Feed a small batch through two sequences of Conv -> Relu -> Max pool. What is the output size now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16, 16, 13, 13])\n",
      "torch.Size([16, 32, 5, 5])\n",
      "torch.Size([16, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(data_example.shape)\n",
    "\n",
    "#1 channel in, 16 channels out\n",
    "out1 = nn.MaxPool2d(kernel_size=2)(nn.ReLU()((nn.Conv2d(1, 16, 3)(data_example))))\n",
    "print(out1.shape)\n",
    "\n",
    "#16 channels in, 32 channels out\n",
    "out2 = nn.MaxPool2d(kernel_size=2)(nn.ReLU()((nn.Conv2d(16, 32, 3)(out1))))\n",
    "print(out2.shape)\n",
    "\n",
    "#32 channels in, 128 channels out\n",
    "out3 = nn.MaxPool2d(kernel_size=2)(nn.ReLU()((nn.Conv2d(32, 128, 3)(out2))))\n",
    "print(out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we want the output layer to have 10 outputs. We can add a linear/dense layer to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2048 x 1], m2: [128 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9d722413aea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2048 x 1], m2: [128 x 10] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "nn.Linear(128, 10)(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Debug and fix this error. Hint: look at dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(128, 10)(Flatten()(out3)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to put all these elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARCHITECTURE\n",
    "image_conv_net = nn.Sequential(nn.Conv2d(1, 16, 3),\n",
    "                               nn.ReLU(),\n",
    "                               nn.MaxPool2d(kernel_size=2),\n",
    "                               \n",
    "                               nn.Conv2d(16, 64, 3),\n",
    "                               nn.ReLU(),\n",
    "                               nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                               nn.Conv2d(64, 128, 3),\n",
    "                               nn.ReLU(),\n",
    "                               nn.MaxPool2d(kernel_size=2),\n",
    "                                                  \n",
    "                               Flatten(),\n",
    "                               nn.Linear(128, 10)\n",
    "                            )\n",
    "\n",
    "#LOSS CRITERION and OPTIMIZER\n",
    "criterion = nn.CrossEntropyLoss() #ensure no softmax in the last layer above\n",
    "optimizer = optim.Adam(image_conv_net.parameters(), lr=1e-2)\n",
    "\n",
    "#DATALOADERS\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, \n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, #shuffle data\n",
    "                                               num_workers=8,\n",
    "                                               pin_memory=True\n",
    "                                             )\n",
    "test_dataloader = torch.utils.data.DataLoader(mnist_test, \n",
    "                                              batch_size=BATCH_SIZE, \n",
    "                                              shuffle=True, #shuffle data\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. Ideally, write a function so we don't have to repeat this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_model(model, train_dataloader, loss_criterion, optimizer, N_epochs = 20):\n",
    "    model.train() #don't worry about this (for this notebook)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(N_epochs):\n",
    "        loss_list = []\n",
    "        for idx, (data_example, data_target) in enumerate(train_dataloader):\n",
    "            data_example = data_example.to(device)\n",
    "            data_target = data_target.to(device)\n",
    "\n",
    "            pred = model(data_example)\n",
    "\n",
    "            loss = loss_criterion(pred, data_target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        if epoch % 5 == 0:        \n",
    "            print(f'Epoch = {epoch} Loss = {np.mean(loss_list)}')    \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 Loss = 0.8495251350800196\n",
      "Epoch = 5 Loss = 0.23720959245165188\n",
      "Epoch = 10 Loss = 0.18100028269489607\n",
      "Epoch = 15 Loss = 0.165368815228343\n"
     ]
    }
   ],
   "source": [
    "image_conv_net = train_image_model(image_conv_net,\n",
    "                                   train_dataloader,\n",
    "                                   criterion,\n",
    "                                   optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a function to do inference and compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_model(model, dataloader):\n",
    "    pred, targets = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #context manager for inference since we don't need the memory footprint of gradients\n",
    "        for idx, (data_example, data_target) in enumerate(dataloader):\n",
    "            data_example = data_example.to(device)\n",
    "\n",
    "            #make predictions\n",
    "            label_pred = model(data_example).argmax(dim=1).float()\n",
    "\n",
    "            #concat and store both predictions and targets\n",
    "            label_pred = label_pred.to('cpu')\n",
    "            \n",
    "            pred = torch.cat((pred, label_pred))\n",
    "            targets = torch.cat((targets, data_target.float()))\n",
    "            \n",
    "    return pred, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred, train_targets = predict_image_model(image_conv_net, train_dataloader)\n",
    "test_pred, test_targets = predict_image_model(image_conv_net, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.9653\n",
      "Test  Accuracy = 0.9240\n"
     ]
    }
   ],
   "source": [
    "assert(train_pred.shape == train_targets.shape)\n",
    "train_accuracy = torch.sum(train_pred == train_targets).item() / train_pred.shape[0]\n",
    "print(f'Train Accuracy = {train_accuracy:.4f}')\n",
    "\n",
    "assert(test_pred.shape == test_targets.shape)\n",
    "test_accuracy = torch.sum(test_pred == test_targets).item() / test_pred.shape[0]\n",
    "print(f'Test  Accuracy = {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the test accuracy went from 96.89% to 97.28%. You might see different numbers due to random initialization of weights and different stochastic batches. Is this significant?\n",
    "\n",
    "**Note**: If you chose a small sample of the data, a convolutional neural net might actually do worse than the feed-forward network.\n",
    "\n",
    "**Question**: Do you think the increase in accuracy is significant? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10,000 examples in the test set. With the feed-forward network, we predicted 9728 examples correctly and with the convolutional net, we predicted 9840 correctly.\n",
    "\n",
    "We can treat the model as a binomial distribution. Recall the binomial distribution describes the number of heads one gets on a coin which has probability $p$ of giving heads and $1-p$ of giving tails if the coin is tossed $N$ times. More formally, the average number of heads will be:\n",
    "\n",
    "$$Np$$\n",
    "\n",
    "and the standard deviation is:\n",
    "\n",
    "$$\\sqrt{Np(1-p)}$$\n",
    "\n",
    "We'll do a rough back-of-the-envelope calculation. Suppose the true $p$ is what our feed-forward network gave us i.e. $p = 0.9728$ and $N = 10,000$.\n",
    "\n",
    "Then, the standard deviation is:\n",
    "\n",
    "$$\\sqrt{10000 * 0.9728 * (1-0.9728)} \\approx 17$$\n",
    "\n",
    "So, to go from 9728 to 9840, we would need ~6.6 standard deviations which is very unlikely. This strongly suggests that the convolutional neural net does give us a significant boost in accuracy as we expected.\n",
    "\n",
    "You can get a sense of the state-of-the-art on MNIST here: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Note: MNIST is generally considered a \"solved\" dataset i.e. it is no longer and hasn't been for a few years, challenging enough as a benchmark for image classification models. You can check out more datasets (CIFAR, Imagenet etc., MNIST on Kannada characters, fashion MNIST etc.) in torchvision.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about preprocessing**: Image pixels takes values between 0 and 255 (inclusive). In the MNIST data here, all the values are scaled down to be between 0 and 1 by dividing by 255. Often it is helpful to subtract the mean for each pixel to help gradient descent converge faster. As an **exercise**, it is highly encouraged to re-train both the feed-forward and convolutional network with zero-mean images.\n",
    "\n",
    "Ensure that the means are computed only on the train set and applied to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come a long way but there's still a lot more to do and see. While we have a lot of labelled data, the vast majority of data is unlabelled. There can be various reasons for this. It might be hard to find experts who can label the data or it is very expensive to do so. So another question is whether we can learn something about a dataset without labels. This is a very broad and difficult field called **unsupervised learning** but we can explore it a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had the MNIST images but no labels. We can no longer build a classification model with it. But we would still like to see if there are broad categories or groups or clusters within the data. Now, we didn't cover techniques like K-means clustering this week but they are definitely an option here. Since this is a class on deep learning, we want to use neural networks.\n",
    "\n",
    "One option is to use networks called **autoencoders**. Since we can't use the labels, we'll instead predict the image itself! In other words, the network takes an image as an input and tries to predict it again. This is the identity mapping:\n",
    "\n",
    "$$i(x) = x$$\n",
    "\n",
    "The trick is to force the network to compress the input. In other words, if we have 784 pixels in the input (and the output), we want the hidden layers to use far less than 784 values. Let's try this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I am being sloppy here by pasting the same training code several times. Ideally, I would abstract away the training and inference pieces in functions inside a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 28x28 image -> 784-dimensional flattened vector\n",
    "#redefining for convenience\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return inp.flatten(start_dim=1, end_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, N_input, N_hidden_nodes):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(Flatten(),\n",
    "                                 nn.Linear(N_input, N_hidden_nodes),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(N_hidden_nodes, N_input),\n",
    "                                 nn.Sigmoid()\n",
    "                                )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.net(inp)\n",
    "        \n",
    "        out = out.view(-1, 28, 28).unsqueeze(1) #return [BATCH_SIZE, 1, 28, 28]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_ae = AE(784, 50) #we are choosing 50 hidden activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (data_example, _) = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data_example.shape)\n",
    "print(image_ff_ae(data_example).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(image_ff_ae.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2326, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(image_ff_ae(data_example), data_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_ae(model, train_dataloader, loss_criterion, optimizer, N_epochs = 20):\n",
    "    model.train() #don't worry about this (for this notebook)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(N_epochs):\n",
    "        loss_list = []\n",
    "        for idx, (data_example, _) in enumerate(train_dataloader):\n",
    "            #Note we don't need the targets/labels here anymore!\n",
    "            data_example = data_example.to(device)\n",
    "\n",
    "            pred = model(data_example)\n",
    "\n",
    "            loss = loss_criterion(pred, data_example)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        if epoch % 5 == 0:        \n",
    "            print(f'Epoch = {epoch} Loss = {np.mean(loss_list)}')    \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 Loss = 0.03667376519739628\n",
      "Epoch = 5 Loss = 0.01722230292111635\n",
      "Epoch = 10 Loss = 0.016654866362611452\n",
      "Epoch = 15 Loss = 0.016621768022576967\n"
     ]
    }
   ],
   "source": [
    "image_ff_ae = train_image_ae(image_ff_ae, train_dataloader, criterion, optimizer, N_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few examples of outputs of our autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ff_ae.to('cpu')\n",
    "output_ae = image_ff_ae(data_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb43bc641d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAO7klEQVR4nO3df5BV9XnH8c/j8quAEBYQESlRirVMU7DdIRpJR+MkQ7At2hlJMLWmYjfTiNFMpomTTEc6sSlTY23IJKYYmZA0mrEjjCRSK0FaijEMC1IBUTEUCiuy0bWCmAALT//YQ2aDe773cu+5P+B5v2Z29t7z3HPPw9XPnnvP957zNXcXgLPfOY1uAEB9EHYgCMIOBEHYgSAIOxDEgHpubJAN9iEaVs9NAqH8Uod11I9Yf7Wqwm5msyR9TVKLpG+7+6LU44domN5v11SzSQAJG3xNbq3it/Fm1iLpG5I+KmmqpHlmNrXS5wNQW9V8Zp8h6RV33+XuRyX9QNKcYtoCULRqwj5B0t4+9/dly36NmbWbWYeZdRzTkSo2B6AaNT8a7+5L3L3N3dsGanCtNwcgRzVh75Q0sc/9C7NlAJpQNWHfKGmKmV1kZoMkfVzSymLaAlC0iofe3L3HzBZI+nf1Dr0tdffthXUGoFBVjbO7+ypJqwrqBUAN8XVZIAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCqOulpHH2aXnPyGT9fWvfyq39ycjNyXW/fMNNybpv4ozq08GeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJw9uJbRrcn6samTkvUL792ZrN9z3qbc2oHjv0iue2Ts0GR9ULKKU7FnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGcPbu/8S5P1jju+VuUW8vcnD705I7nmoCc3Vrlt9FVV2M1st6RDko5L6nH3tiKaAlC8IvbsV7v76wU8D4Aa4jM7EES1YXdJT5nZJjNr7+8BZtZuZh1m1nFMR6rcHIBKVfs2fqa7d5rZeZJWm9mL7r6u7wPcfYmkJZI0wlq9yu0BqFBVe3Z378x+d0laISl9eBVAw1QcdjMbZmbnnrwt6SOSthXVGIBiVfM2fpykFWZ28nkedvcnC+kKhem+5Ypk/bHb7i3xDJw1fraoOOzuvkvStAJ7AVBDDL0BQRB2IAjCDgRB2IEgCDsQBKe4ngXe+sTlubUVC9NDa2NbBifri99MnwK79mPpEx0PzMy/VPXhCyy57iT9JFnH6WHPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM5+BkiNo0vST+/9Vm7tmP9Gct2rt96QrA+btStZl15KVsdsT9RKPDOKxZ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0JlLrcc6lz0lNj6Zc+fWty3UvvfjNZ70lWcSZhzw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDOXgdv3JoeR3/4b76arJe6tvvUf1mQW7v0n/cn1+3ZtTtZx9mj5J7dzJaaWZeZbeuzrNXMVpvZzuz3qNq2CaBa5byN/46kWacsu0vSGnefImlNdh9AEysZdndfJ6n7lMVzJC3Lbi+TdF3BfQEoWKWf2ce5+8kPg69JGpf3QDNrl9QuSUM0tMLNAahW1Ufj3d0leaK+xN3b3L1toNIHmgDUTqVhP2Bm4yUp+91VXEsAaqHSsK+UdHN2+2ZJjxfTDoBaKfmZ3cwekXSVpDFmtk/S3ZIWSXrUzOZL2iNpbi2bbHYDzs89ZCFJuuGOHyfrkwYMStZLzZF+8Reeza018/nopV63zrmTk/VL5qavWb9l34Tc2rlPD0uuO2ZJ/mt6pioZdnefl1O6puBeANQQX5cFgiDsQBCEHQiCsANBEHYgCE5xLcBL912QrC9v/VGyXmpo7T+unVqig70l6pXzK6Yl63uuTX8F+sEbH8itDbFjyXWnpUckNe2ZW5L1ton5r8uXv/TD5Lp/OvTzyfr5//STZL0ZsWcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZy+TDch/qcaMOpRc95wSf1OXvXx5sj5hz/ZkPaXlty5K1rsXtyTri3/n28n6jctvT9b/7uLpyXo1Jmlrsv5Gonb72vRZ2R1//fVk/apXb0vWhz/602S9EdizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLOXqWVi/mWJ/3PaI8l1T5R47kFPjaigo/K8cuv5yfr0ES8n6397xR8l65Nfa77x5HLYn6Xr29enL8L98+t/kayP+Ldzk/UTh9LfzagF9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7HXw3JH039Rx699M1kuN06cMuuRgst65eEqyPvwMHUcvpafz1WS9feGdyfrWryxO1v/49/4yWbdntiTrtVByz25mS82sy8y29Vm20Mw6zWxL9jO7tm0CqFY5b+O/I2lWP8vvd/fp2c+qYtsCULSSYXf3dZK669ALgBqq5gDdAjN7PnubPyrvQWbWbmYdZtZxTEeq2ByAalQa9gckTZY0XdJ+SfflPdDdl7h7m7u3DdTgCjcHoFoVhd3dD7j7cXc/IelBSTOKbQtA0SoKu5mN73P3eknb8h4LoDmUHGc3s0ckXSVpjJntk3S3pKvMbLokl7Rb0qdq2OMZ7yt7r03WT2x7sWbbHvJE+lz5kWteStaPF9nMGWT08vT+67MLPpis7/zzgcn6Jc+cdktVKxl2d5/Xz+KHatALgBri67JAEIQdCIKwA0EQdiAIwg4EwSmudTB7bHpq4R9OvCxZ79m7r+Jtj37o2WQ96tBaKaUu9fzqO+OS9aFj3imynUKwZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnL9eJ/As6dx9PX25r/sj/TdaXfusDyXrrnPR/Ju9JTy+M03fO9KnJ+o3jn0jW71n5iSLbKQR7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2MvXs2Ztb+8CTn02u++K130zW/2v6w8n6Bx+/MVkf++lf5tZSfZ/tBkyamFs7PPX85Lq33L8iWX/nRHp2owuefitZ92S1NtizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ5l6/Eb8R1urvt2vqtr16aRndmqy//PXfTNZXXfmNZH3SgEHJ+rKDk/Kfu+t9yXV/9sTkZH38+sPJ+oC38sf4Szm+PT1ddM+H/iBZf/0z6Wuz/9Ul63Jrpa4x8D896X/Xpz95e7LesnZzsl4rG3yNDnq39VcruWc3s4lmttbMXjCz7WZ2R7a81cxWm9nO7PeoohsHUJxy3sb3SPqcu0+VdLmk28xsqqS7JK1x9ymS1mT3ATSpkmF39/3uvjm7fUjSDkkTJM2RtCx72DJJ19WqSQDVO63vxpvZeyVdJmmDpHHuvj8rvSap38mvzKxdUrskDdHQSvsEUKWyj8ab2XBJj0m6090P9q1571G+fo/0ufsSd29z97aBSp88AKB2ygq7mQ1Ub9C/7+7Ls8UHzGx8Vh8vqas2LQIoQsmhNzMz9X4m73b3O/ssv1fSG+6+yMzuktTq7p9PPdfZOvRWre6/uCJZ/9Bn0tMu33PeptzaCeVfArscO46m1+86PjxZb7H89Z89PCW57tyR+f8uqfSQ5Mzn8i/n3P1/w5LrTrnvaLLuz21P1hslNfRWzmf2KyXdJGmrmW3Jln1R0iJJj5rZfEl7JM0tolkAtVEy7O6+XlK/fykksZsGzhB8XRYIgrADQRB2IAjCDgRB2IEgOMX1DNDynpHJ+o5Fv13xc//91f+arG98+6KKn1uSlm9sq2r9lCnL0lNl24Zt+cUTxwvupjlUdYorgLMDYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTg7cBZhnB0AYQeiIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRRMuxmNtHM1prZC2a23czuyJYvNLNOM9uS/cyufbsAKlXO/Ow9kj7n7pvN7FxJm8xsdVa7392/Wrv2ABSlnPnZ90van90+ZGY7JE2odWMAinVan9nN7L2SLpO0IVu0wMyeN7OlZjYqZ512M+sws45jSk/XA6B2yg67mQ2X9JikO939oKQHJE2WNF29e/77+lvP3Ze4e5u7tw3U4AJaBlCJssJuZgPVG/Tvu/tySXL3A+5+3N1PSHpQ0ozatQmgWuUcjTdJD0na4e7/2Gf5+D4Pu15SYspMAI1WztH4KyXdJGmrmW3Jln1R0jwzmy7JJe2W9KmadAigEOUcjV8vqb/rUK8qvh0AtcI36IAgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GYu9dvY2Y/l7Snz6Ixkl6vWwOnp1l7a9a+JHqrVJG9TXL3sf0V6hr2d23crMPd2xrWQEKz9tasfUn0Vql69cbbeCAIwg4E0eiwL2nw9lOatbdm7Uuit0rVpbeGfmYHUD+N3rMDqBPCDgTRkLCb2Swze8nMXjGzuxrRQx4z221mW7NpqDsa3MtSM+sys219lrWa2Woz25n97neOvQb11hTTeCemGW/oa9fo6c/r/pndzFokvSzpw5L2SdooaZ67v1DXRnKY2W5Jbe7e8C9gmNkfSnpb0nfd/XezZf8gqdvdF2V/KEe5+xeapLeFkt5u9DTe2WxF4/tOMy7pOkmfVANfu0Rfc1WH160Re/YZkl5x913uflTSDyTNaUAfTc/d10nqPmXxHEnLstvL1Ps/S93l9NYU3H2/u2/Obh+SdHKa8Ya+dom+6qIRYZ8gaW+f+/vUXPO9u6SnzGyTmbU3upl+jHP3/dnt1ySNa2Qz/Sg5jXc9nTLNeNO8dpVMf14tDtC920x3/31JH5V0W/Z2tSl572ewZho7LWsa73rpZ5rxX2nka1fp9OfVakTYOyVN7HP/wmxZU3D3zux3l6QVar6pqA+cnEE3+93V4H5+pZmm8e5vmnE1wWvXyOnPGxH2jZKmmNlFZjZI0sclrWxAH+9iZsOyAycys2GSPqLmm4p6paSbs9s3S3q8gb38mmaZxjtvmnE1+LVr+PTn7l73H0mz1XtE/meSvtSIHnL6uljSf2c/2xvdm6RH1Pu27ph6j23MlzRa0hpJOyX9WFJrE/X2PUlbJT2v3mCNb1BvM9X7Fv15SVuyn9mNfu0SfdXldePrskAQHKADgiDsQBCEHQiCsANBEHYgCMIOBEHYgSD+H4BJeWul6ZrJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 15 #change this to see different examples\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(data_example[idx][0].detach().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(output_ae[idx][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, great - we have a neural network that can predict the input from the input. Is this useful? Recall that we had an intermediate layer that had 50 activations. Feel free to change this number around and see what happens.\n",
    "\n",
    "We are compressing 784 pixel values into 50 activations and then reconstructing the image from those 50 values. In other words, we are forcing the neural network to capture only relevant non-linear features that can help it remember what image the input was.\n",
    "\n",
    "The compression is not perfect as you can see in the reconstructed image above but it's pretty good. Training for more time or better training methods might improve this.\n",
    "\n",
    "So how exactly is this useful. Maybe:\n",
    "\n",
    "* Using an autoencoder to do lossy compression. Image storing the 50 activations instead of each image and storing the last layer (the \"decoder\") that constructs the image from the 50 activations.\n",
    "\n",
    "* For search: suppose we wanted to search for a target image in a database of N images. We could do N pixel-by-pixel matches but these won't work because even a slight change in position or orientation or pixel intensities will give misleading distances between images. But if we use the vector of intermediate (50, in this case) activations, then maybe we can do a search in the space of activations. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#full mnist data\n",
    "print(mnist_train.data.float().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally it's a good idea to split the forward function into an encoder and decoder function. Here we do it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=50, out_features=784, bias=True)\n",
       "  (4): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ff_ae.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the activations after the hidden relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mnist_ae_act = image_ff_ae.net[2](image_ff_ae.net[1](image_ff_ae.net[0](mnist_train.data.float())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 50])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick some example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb43bb6ddd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_idx = 15 #between 0 and 60000-1\n",
    "\n",
    "plt.imshow(mnist_train.data[img_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the target image activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img_act = mnist_ae_act[img_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.0000,    0.0000,  344.1409,    0.0000, 1460.8500,  817.6396,\n",
       "           0.0000,    0.0000,    0.0000,  780.8979,    0.0000,    0.0000,\n",
       "         704.9445,    0.0000,    0.0000,  476.1716,  292.7066,    0.0000,\n",
       "         599.8943,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
       "        1457.6251,    0.0000, 2844.3250,    0.0000,    0.0000,    0.0000,\n",
       "         605.9978,   61.5421,    0.0000, 2161.3923,    0.0000,    0.0000,\n",
       "           0.0000,    0.0000,  438.2827,  369.7802,    0.0000,    0.0000,\n",
       "           0.0000,  709.2001,    0.0000,  717.5428,  414.9775,  412.0234,\n",
       "         483.0129,    0.0000])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_img_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cosine distance between two vectors to find the nearest neighbors. \n",
    "\n",
    "**Question**: Can you think of an elegant matrix-operation way of implementing this (so it can also run on a GPU)?\n",
    "\n",
    "**Warning**: Always keep an eye out for memory usage. The full matrix of pairwise distances can be very large. Work with a subset of the data (even 100 images) if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save memory, look at only first N images (1000 here)\n",
    "mnist_ae_act = mnist_ae_act[0:1000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine distance between two points, $\\vec{x}_i, \\vec{x}_j$ is:\n",
    "\n",
    "$$d_{ij} = \\frac{\\vec{x}_i . \\vec{x}_j}{\\lVert \\vec{x}_i \\rVert \\lVert \\vec{x}_j \\rVert}$$\n",
    "\n",
    "Now we can first normalize all the actiation vector so they have length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(mnist_ae_act, 2).sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't divide a tensor of shape [60000, 50] (activations) by a tensor of shape [60000].\n",
    "\n",
    "So first we have to unsqueeze (add an additional dimension) to get a shape [60000,1] and then broadcast/expand as the target tensor.\n",
    "\n",
    "We should check that the first row contains the length of the first image's activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14569491., 14569491., 14569491.,  ..., 14569491., 14569491.,\n",
       "         14569491.],\n",
       "        [26621508., 26621508., 26621508.,  ..., 26621508., 26621508.,\n",
       "         26621508.],\n",
       "        [44020636., 44020636., 44020636.,  ..., 44020636., 44020636.,\n",
       "         44020636.],\n",
       "        ...,\n",
       "        [15814138., 15814138., 15814138.,  ..., 15814138., 15814138.,\n",
       "         15814138.],\n",
       "        [19887868., 19887868., 19887868.,  ..., 19887868., 19887868.,\n",
       "         19887868.],\n",
       "        [44205368., 44205368., 44205368.,  ..., 44205368., 44205368.,\n",
       "         44205368.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(mnist_ae_act, 2).sum(dim=1).unsqueeze(1).expand_as(mnist_ae_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide by the norm (don't forget the sqrt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ae_act_norm = mnist_ae_act / torch.pow(torch.pow(mnist_ae_act, 2).sum(dim=1).unsqueeze(1).expand_as(mnist_ae_act), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.0000,    0.0000,    0.0000,    0.0000, 1373.7821, 1806.4620,\n",
       "           0.0000,    0.0000,    0.0000, 1539.6263,    0.0000,    0.0000,\n",
       "         732.2988,    0.0000,    0.0000, 1489.2239,  389.7503,  425.1606,\n",
       "         990.7882,    0.0000,    0.0000,    0.0000,    0.0000, 1432.6848,\n",
       "        1173.0520,    0.0000,  909.7496,    0.0000,    0.0000,    0.0000,\n",
       "           0.0000,  701.1577,    0.0000,  477.3211,    0.0000,    0.0000,\n",
       "           0.0000,    0.0000, 1125.9907, 1101.0918,    0.0000,    0.0000,\n",
       "           0.0000, 1666.0519,  129.9131,    0.0000,   12.4162, 1514.1362,\n",
       "        1475.1205,    0.0000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5129.0112)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.pow(mnist_ae_act[10], 2).sum(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.2678, 0.3522, 0.0000, 0.0000, 0.0000,\n",
       "        0.3002, 0.0000, 0.0000, 0.1428, 0.0000, 0.0000, 0.2904, 0.0760, 0.0829,\n",
       "        0.1932, 0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.2287, 0.0000, 0.1774,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.0000, 0.0931, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2195, 0.2147, 0.0000, 0.0000, 0.0000, 0.3248, 0.0253,\n",
       "        0.0000, 0.0024, 0.2952, 0.2876, 0.0000])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act[10] / torch.pow(torch.pow(mnist_ae_act[10], 2).sum(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.2678, 0.3522, 0.0000, 0.0000, 0.0000,\n",
       "        0.3002, 0.0000, 0.0000, 0.1428, 0.0000, 0.0000, 0.2904, 0.0760, 0.0829,\n",
       "        0.1932, 0.0000, 0.0000, 0.0000, 0.0000, 0.2793, 0.2287, 0.0000, 0.1774,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.0000, 0.0931, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2195, 0.2147, 0.0000, 0.0000, 0.0000, 0.3248, 0.0253,\n",
       "        0.0000, 0.0024, 0.2952, 0.2876, 0.0000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act_norm[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! They are the same. We have confidence that we are normalizing the activation vectors correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the cosine distance is:\n",
    "\n",
    "$$d_{ij} = \\vec{x}_i . \\vec{x}_j$$\n",
    "\n",
    "since all the vectors are of unit length.\n",
    "\n",
    "**Question**: How would you compute this using matrix operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1000])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act_norm.transpose(1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ae_act_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_pairwise_cosine = torch.mm(mnist_ae_act_norm, mnist_ae_act_norm.transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_pairwise_cosine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_pairwise_cosine[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_idx = 18 #between 0 and 60000-1\n",
    "\n",
    "plt.imshow(mnist_train.data[img_idx])\n",
    "plt.title(\"Target image\")\n",
    "\n",
    "#find closest image\n",
    "top5 = torch.sort(ae_pairwise_cosine[img_idx], descending=True) #or use argsort\n",
    "top5_vals = top5.values[0:5]\n",
    "top5_idx = top5.indices[0:5]\n",
    "\n",
    "for i, idx in enumerate(top5_idx):\n",
    "    plt.figure()\n",
    "    plt.imshow(mnist_train.data[idx])\n",
    "    if i==0:\n",
    "        plt.title(\"Sanity check : same as input\")\n",
    "    else:\n",
    "        plt.title(f\"match {i} : cosine = {top5_vals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a simple dataset and a simple autoencoder, we already have some pretty good anecdotal similarity searches. There are many variations on autoencoders from switching layers to adding noise to the inputs (denoising autoencoders) to adding sparsity penalties to the hidden layer activations to encourage sparse activations to graphical models called variational autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete activations and cosine distances to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ae_act = None\n",
    "mnist_ae_act_norm = None\n",
    "ae_pairwise_cosine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "By now, you have had quite some experience with writing your own neural networks and introspecting into what they are doing. We still haven't touched topics like recurrent neural networks, seq2seq models and more modern applications. They will get added to this notebook so if you are interested, please revisit the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Items\n",
    "\n",
    "Real problems  \n",
    "    \n",
    "    MNIST + autoencoder (convnet)\n",
    "    \n",
    "Trip Classification:\n",
    "    \n",
    "    Maybe?\n",
    "    \n",
    "RNN toy problems\n",
    "\n",
    "    Linear trend + noise\n",
    "    \n",
    "    Different data structuring strategies\n",
    "    \n",
    "    Quadratic trend + noise\n",
    "    \n",
    "    LSTM/GRUs for same problems\n",
    "    \n",
    "Seq2Seq examples\n",
    "    \n",
    "RNN Autoencoder\n",
    "\n",
    "    What data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (In progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You might have run into memory issues by now. Everything below is self contained so if you want to reset the notebook and start from the cell below, it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's generate some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rnn_data(N_examples=1000, noise_var = 0.1, lag=1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    ts = 4 + 3*np.arange(N_examples) + np.random.normal(0, noise_var)        \n",
    "        \n",
    "    features = ts[0:len(ts)-lag]\n",
    "    target = ts[lag:]\n",
    "    \n",
    "    return features, target    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = generate_rnn_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is possibly the simplest time-series one could pick (apart from a constant value). It's a simple linear trend with a tiny bit of gaussian noise. Note that this is a **non-stationary** series!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features, 'p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the series at time t+1 given the value at time t (and history).\n",
    "\n",
    "Of course, we could try using a feed-forward network for this. But instead, we'll use this to introduce recurrent neural networks.\n",
    "\n",
    "Recall that the simplest possible recurrent neural network has a hidden layer that evolves in time, $h_t$, inputs $x_t$ and outputs $y_t$.\n",
    "\n",
    "$$h_t = \\sigma(W_{hh} h_{t-1} + W_{hx} x_t + b_h)$$\n",
    "\n",
    "with outputs:\n",
    "\n",
    "$$y_t = W_{yh} h_t + b_y$$\n",
    "\n",
    "Since the output is an unbounded real value, we won't have an activation on the output.\n",
    "\n",
    "Let's write our simple RNN. This is not general - we don't have the flexibility of adding more layers (as discussed in the lecture), bidirectionality etc. but we are in experimental mode so it's okay. Eventually, you can use pytorch's in-built torch.nn.RNN class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_input = 1 #will pass only one value as input\n",
    "N_output = 1 #will predict one value\n",
    "\n",
    "N_hidden = 32 #number of hidden dimensions to use\n",
    "\n",
    "hidden_activation = nn.ReLU()\n",
    "\n",
    "#define weights and biases\n",
    "w_hh = nn.Parameter(data = torch.Tensor(N_hidden, N_hidden), requires_grad = True)\n",
    "w_hx = nn.Parameter(data = torch.Tensor(N_hidden, N_input), requires_grad = True)\n",
    "w_yh = nn.Parameter(data = torch.Tensor(N_output, N_hidden), requires_grad = True)\n",
    "\n",
    "b_h = nn.Parameter(data = torch.Tensor(N_hidden, 1), requires_grad = True)\n",
    "b_y = nn.Parameter(data = torch.Tensor(N_output, 1), requires_grad = True)\n",
    "\n",
    "#initialize weights and biases (in-place)\n",
    "nn.init.kaiming_uniform_(w_hh)\n",
    "nn.init.kaiming_uniform_(w_hx)\n",
    "nn.init.kaiming_uniform_(w_yh)\n",
    "\n",
    "nn.init.zeros_(b_h)\n",
    "nn.init.zeros_(b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_act = hidden_activation(torch.mm(w_hx, torch.ones(N_input, 1)) + \\\n",
    "                               torch.mm(w_hh, torch.ones(N_hidden, 1)) + \\\n",
    "                               b_h)\n",
    "print(hidden_act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (torch.mm(w_yh, hidden_act) + b_y)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the input we'll be passing will be a time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ts = torch.Tensor([1,2,3]).unsqueeze(1).unsqueeze(2)\n",
    "print(inp_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_act = torch.zeros(N_hidden, 1)\n",
    "\n",
    "#-----------first iter--------\n",
    "hidden_act = hidden_activation(torch.mm(w_hx, inp_ts[0]) + \\\n",
    "                               torch.mm(w_hh, hidden_act) + \\\n",
    "                               b_h)\n",
    "print(hidden_act.shape)\n",
    "\n",
    "output = (torch.mm(w_yh, hidden_act) + b_y)\n",
    "print(output)\n",
    "\n",
    "#-----------second iter--------\n",
    "hidden_act = hidden_activation(torch.mm(w_hx, inp_ts[1]) + \\\n",
    "                               torch.mm(w_hh, hidden_act) + \\\n",
    "                               b_h)\n",
    "print(hidden_act.shape)\n",
    "\n",
    "output = (torch.mm(w_yh, hidden_act) + b_y)\n",
    "print(output)\n",
    "\n",
    "#-----------third iter--------\n",
    "hidden_act = hidden_activation(torch.mm(w_hx, inp_ts[2]) + \\\n",
    "                               torch.mm(w_hh, hidden_act) + \\\n",
    "                               b_h)\n",
    "print(hidden_act.shape)\n",
    "\n",
    "output = (torch.mm(w_yh, hidden_act) + b_y)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_act = torch.zeros(N_hidden, 1)\n",
    "\n",
    "for x in inp_ts: #input time-series \n",
    "    hidden_act = hidden_activation(torch.mm(w_hx, x) + \\\n",
    "                                   torch.mm(w_hh, hidden_act) + \\\n",
    "                                   b_h)\n",
    "    print(hidden_act.shape)\n",
    "\n",
    "    output = (torch.mm(w_yh, hidden_act) + b_y)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, N_input, N_hidden, N_output, hidden_activation):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.N_input = N_input\n",
    "        self.N_hidden = N_hidden\n",
    "        self.N_output = N_output\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        #define weights and biases\n",
    "        self.w_hh = nn.Parameter(data = torch.Tensor(N_hidden, N_hidden), requires_grad = True)\n",
    "        self.w_hx = nn.Parameter(data = torch.Tensor(N_hidden, N_input), requires_grad = True)\n",
    "        self.w_yh = nn.Parameter(data = torch.Tensor(N_output, N_hidden), requires_grad = True)\n",
    "\n",
    "        self.b_h = nn.Parameter(data = torch.Tensor(N_hidden, 1), requires_grad = True)\n",
    "        self.b_y = nn.Parameter(data = torch.Tensor(N_output, 1), requires_grad = True)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.kaiming_uniform_(self.w_hh)\n",
    "        nn.init.kaiming_uniform_(self.w_hx)\n",
    "        nn.init.kaiming_uniform_(self.w_yh)\n",
    "\n",
    "        nn.init.zeros_(self.b_h)\n",
    "        nn.init.zeros_(self.b_y)                        \n",
    "            \n",
    "    def forward(self, inp_ts, hidden_act=None):\n",
    "        if hidden_act is None:\n",
    "            #initialize to zero if hidden not passed\n",
    "            hidden_act = torch.zeros(self.N_hidden, 1)\n",
    "            \n",
    "\n",
    "        output_vals = torch.tensor([])\n",
    "        for x in inp_ts: #input time-series \n",
    "            hidden_act = self.hidden_activation(torch.mm(self.w_hx, x) + \\\n",
    "                                                torch.mm(self.w_hh, hidden_act) + \\\n",
    "                                                self.b_h)\n",
    "\n",
    "            output = (torch.mm(self.w_yh, hidden_act) + self.b_y)\n",
    "            output_vals = torch.cat((output_vals, output))\n",
    "            \n",
    "        return output_vals, hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(N_input, N_hidden, N_output, hidden_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vals, hidden_act = rnn(inp_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vals)\n",
    "print(\"---------\")\n",
    "print(hidden_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. Now how do we actually tune the weights? As before, we want to compute a loss between the predictions from the RNN and the labels. Once we have a loss, we can do the usual backpropagation and gradient descent.\n",
    "\n",
    "Recall that our \"features\" are:\n",
    "\n",
    "\n",
    "$$x_1, x_2, x_3\\ldots$$\n",
    "\n",
    "Our \"targets\" are:\n",
    "\n",
    "$$x_2, x_3, x_4 \\ldots$$\n",
    "\n",
    "if the lag argument in generate_rnn_data is 1. More generally, it would be:\n",
    "\n",
    "$$x_{1+\\text{lag}}, x_{2+\\text{lag}}, x_{3+\\text{lag}}, \\ldots$$\n",
    "\n",
    "Now, let's focus on the operational aspects for a second. In principle, you would first feed $x_1$ as an input, generate an **estimate** for $\\hat{x}_2$ as the output.\n",
    "\n",
    "Ideally, this would be close to the actual value $x_2$ but that doesn't have to be the case, especially when the weights haven't been tuned yet. Now, for the second step, we need to input $x_2$ to the RNN. The question is whether we should use $\\hat{x}_2$ or $x_2$.\n",
    "\n",
    "In real-life, one can imagine forecasting a time-series into the future given values till time t. In this case, we would have to feed our prediction at time t, $\\hat{x}_{t+1}$ as input at the next time-step since we don't know $x_{t+1}$.\n",
    "\n",
    "The problem with this approach is that errors start compounding really fast. While we might be a bit off at $t+1$, if our prediction $\\hat{x}_{t+1}$ is inaccurate, then our prediction $\\hat{x}_{t+2}$ will be even worse and so on.\n",
    "\n",
    "In our case, we'll use what's called **teacher forcing**. We'll always feed the actual known $x_t$ at time-step t instead of the prediction from the previous time-step, $\\hat{x}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Split the features and target into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = len(features)\n",
    "\n",
    "TRAIN_PERC = 0.70\n",
    "\n",
    "TRAIN_SPLIT = int(TRAIN_PERC * N_examples)\n",
    "\n",
    "features_train = features[:TRAIN_SPLIT]\n",
    "target_train = target[:TRAIN_SPLIT]\n",
    "\n",
    "features_test = features[TRAIN_SPLIT:]\n",
    "target_test = target[:TRAIN_SPLIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate([features_train, features_test]))\n",
    "plt.plot(features_train, label='train')\n",
    "plt.plot(np.arange(len(features_train)+1, len(features)+1), features_test, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_input = 1 #will pass only one value as input\n",
    "N_output = 1 #will predict one value\n",
    "N_hidden = 32 #number of hidden dimensions to use\n",
    "\n",
    "hidden_activation = nn.ReLU()\n",
    "\n",
    "rnn = RNN(N_input, N_hidden, N_output, hidden_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.tensor(features_train).unsqueeze(1).unsqueeze(2)\n",
    "target_train = torch.tensor(target_train).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "features_test = torch.tensor(features_test).unsqueeze(1).unsqueeze(2)\n",
    "target_test = torch.tensor(target_test).unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vals, hidden_act = rnn(features_train.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_vals))\n",
    "print(len(target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.tensor(output_vals).double(), target_train.squeeze(2).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put all these ingredients together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_input = 1 #will pass only one value as input\n",
    "N_output = 1 #will predict one value\n",
    "N_hidden = 4 #number of hidden dimensions to use\n",
    "\n",
    "hidden_activation = nn.Tanh()\n",
    "\n",
    "rnn = RNN(N_input, N_hidden, N_output, hidden_activation)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=1e-1)\n",
    "\n",
    "N_epochs = 10000\n",
    "\n",
    "hidden_act = None\n",
    "\n",
    "for n in range(N_epochs):\n",
    "    output_vals, hidden_act = rnn(features_train.float(), hidden_act = None)\n",
    "    \n",
    "    loss = criterion(output_vals, target_train.squeeze(1).float())\n",
    "    \n",
    "    #loss.requires_grad = True\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(rnn.w_yh.grad)\n",
    "        print(f'loss = {loss}')\n",
    "        print(output_vals.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output_vals, target_train.squeeze(1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i.item() for i in output_vals])\n",
    "plt.plot([i[0] for i in target_train.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.w_hh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.requires_grad = True\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.w_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.w_hx.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
