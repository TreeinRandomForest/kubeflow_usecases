{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eb429a0738c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Spark imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Spark imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data from CSV file to S3 Bucket\n",
    "\n",
    "In most cases, this step will be done by your organization and as a data scientist, you'll be accessing the pre-uploaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 bucket coordinates and credentials\n",
    "try:\n",
    "    s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "    s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "    s3_endpoint_url = os.environ['ENDPOINT_URL']\n",
    "except:\n",
    "    raise ValueError('Keys not found in os.environ Please set credentials and endpoint URL as environment variables.')\n",
    "\n",
    "s3_bucket = \"FRAUDDETECTION\"\n",
    "\n",
    "#Create S3 client and bucket\n",
    "s3 = boto3.client(service_name = 's3',\n",
    "                  aws_access_key_id = s3_access_key,\n",
    "                  aws_secret_access_key = s3_secret_key, \n",
    "                  endpoint_url=s3_endpoint_url)\n",
    "s3.create_bucket(Bucket = s3_bucket)\n",
    "\n",
    "#Upload data to bucket with key.\n",
    "key = \"uploaded/creditcard-sample10k.csv\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"creditcard-sample10k.csv\")\n",
    "\n",
    "#Check data exists in bucket\n",
    "prefix='uploaded/'\n",
    "s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from S3 storage using Spark\n",
    "\n",
    "Spark stores data using a construct called a **resilient distributed dataset** (RDD) which distributes the data across nodes in a cluster. This distribution is done in-memory (reading from memory is orders-of-magnitude faster for memory compared to disk) and is done in a fault-tolerant way so that data is recoverable even if some of the nodes in the cluster crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please set environment variable \"SPARK_CLUSTER\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d0a152e5855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Setup a Spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'SPARK_CLUSTER'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please set environment variable \"SPARK_CLUSTER\".'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark_cluster_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_cluster_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please set environment variable \"SPARK_CLUSTER\"."
     ]
    }
   ],
   "source": [
    "#Setup a Spark session\n",
    "if 'SPARK_CLUSTER' not in os.environ:\n",
    "    raise ValueError('Please set environment variable \"SPARK_CLUSTER\".')\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "print(spark_cluster_url)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"odh-pyspark\").master(spark_cluster_url).getOrCreate() #get/create a session\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "print(spark.sparkContext.version)\n",
    "print(\"Spark Session created successfully.\")\n",
    "\n",
    "#Setup access for the Spark session to the S3 storage endpoint\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\");\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key) \n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", 'false')\n",
    "\n",
    "print(\"Reading data using Spark...\")\n",
    "df = spark.read.format(\"csv\")\\\n",
    "               .option(\"header\", \"true\")\\\n",
    "               .option(\"inferSchema\", \"True\")\\\n",
    "               .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "               .load(f\"s3a://{s3_bucket}/uploaded/creditcard-sample10k.csv\")\n",
    "\n",
    "print(f'Dataframe type: {type(df)}')\n",
    "print(f\"Total number of credit card transaction rows: {df.count()}\")\n",
    "\n",
    "#You can treat a Spark dataframe just as a Pandas dataframe for many operations\n",
    "print(\"Total number of rows with fraud\")\n",
    "print(df[(df['Class']==1)].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Credit Card Data to ODH Ceph-Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_endpoint_url = os.environ['ENDPOINT_URL']\n",
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "#s3_bucket = os.environ['BUCKET']\n",
    "s3_bucket=\"FRAUDDETECTION\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id = s3_access_key,aws_secret_access_key = s3_secret_key, endpoint_url=s3_endpoint_url)\n",
    "s3.create_bucket(Bucket=s3_bucket)\n",
    "\n",
    "# Upload to Rook/Ceph in bucket Open and key uploaded/creditcard-sample10k.csv\n",
    "key = \"uploaded/creditcard-sample10k.csv\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"creditcard-sample10k.csv\")\n",
    "\n",
    "prefix='uploaded/'\n",
    "result = s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')\n",
    "print(result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import boto3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve\n",
    "        \n",
    "print(\"Getting a spark session with the distributed spark cluster running on Openshift \")\n",
    "\n",
    "#Spark Session\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "print(spark_cluster_url)\n",
    "spark = SparkSession.builder.appName(\"odh-pyspark\").master(spark_cluster_url).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "print(spark.sparkContext.version)\n",
    "print(\"Spark Session Success\")\n",
    "\n",
    "#Set the Hadoop configurations to access Ceph S3\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\");\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key) \n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", 'false')\n",
    "\n",
    "print(\"Spark reading transaction data\")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(f\"s3a://{s3_bucket}/uploaded/creditcard-sample10k.csv\")\n",
    "\n",
    "print(\"Total number of credit card transaction rows: %d\" % df.count())\n",
    "### Check the total number of rows with fraud is detected\n",
    "print(\"Total number of rows with fraud\")\n",
    "print(df[(df['Class']==1)].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Order the credit card transaction by transaction time\n",
    "df.orderBy(\"Time\")\n",
    "\n",
    "#number of rows in the dataset\n",
    "n_samples = df.count()\n",
    "print(n_samples)\n",
    "\n",
    "#Split into train and test\n",
    "train_size = 0.75\n",
    "\n",
    "train_limit = int(n_samples * train_size)\n",
    "df_train = df.limit(train_limit)     \n",
    "df_test = df.subtract(df_train) \n",
    "\n",
    "#Data Schema\n",
    "print(\"Original Data Schema\")\n",
    "df_test.printSchema()\n",
    "\n",
    "print('Number of train transactions: %s', df_train.count())\n",
    "print('Number of test  transactions: %s', df_test.count())\n",
    "\n",
    "#Define features and target variables for convenience.\n",
    "drop_time_class = ['_c0', 'Time', 'Class']\n",
    "drop_class=['Class']\n",
    "\n",
    "#Create Train Datasets\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "#Create Test Datasets\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "\n",
    "#Create a RondomForest Classifier mode\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "\n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(train_labels, train_pred):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    labels = list(train_labels['Class'].value_counts().index)\n",
    "    print(labels)\n",
    "\n",
    "    confusion = confusion_matrix(train_labels, train_pred, labels=labels)\n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation=90);\n",
    "    ax.set_yticklabels(labels);\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted')    \n",
    "    plt.ylabel('true')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(target_train_pd['Class'].value_counts())\n",
    "\n",
    "_=plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_=plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "feat_imp = sorted(zip(features_train_pd.columns, model.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in feat_imp], [i[1] for i in feat_imp], 'p-')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target variables for convenience.\n",
    "## From the graph we only want seven important features V3,V4,V10,V11,V12,V14,V17\n",
    "drop_time_class = ['_c0', 'Time', 'Class','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "drop_class=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "features_test.printSchema()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=6, n_jobs=10, class_weight='balanced')\n",
    "                               \n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))\n",
    "  \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_ = plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = ['V3','V4','V10','V11','V12','V14','V17','Amount'])\n",
    "    print(model.predict(rowdf))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Rook/Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"model.pkl\")\n",
    "prefix='uploaded/'\n",
    "result = s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenShift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o oc.tar.gz -L https://mirror.openshift.com/pub/openshift-v3/clients/4.0.22/linux/oc.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp oc ~/../bin/oc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login into Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc login -u <INSERT USERNAME> -p <INSERT PASSWORD> --insecure-skip-tls-verify <INSERT CLUSTER URL>:6443\n",
    "oc project frauddetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model With Seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project frauddetection\n",
    "oc create -n frauddetection -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/mymodel.json\n",
    "oc get seldondeployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model in Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp jq ~/../bin/jq\n",
    "chmod 777 ~/../bin/jq\n",
    "export TOKENJSON=$(curl -XPOST -u oauth-key:oauth-secret <INSERT SELDON API SERVER URL>/oauth/token -d 'grant_type=client_credentials')\n",
    "export TOKEN=$(echo $TOKENJSON | jq \".access_token\" -r)\n",
    "echo $TOKEN\n",
    "\n",
    "curl -v --header \"Authorization: Bearer $TOKEN\" <INSERT SELDON API SERVER URL>/api/v0.1/predictions -d '{\"strData\": \"0.365194527642578,0.819750231339882,-0.5927999453145171,-0.619484351930421,-2.84752569239798,1.48432160780265,0.499518887687186,72.98\"}' -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the served model from python using the test dataframe\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Get the token\n",
    "post_data = {\"grant_type\": \"client_credentials\"}\n",
    "requestOauth = requests.post('<INSERT SELDON API SERVER URL>/oauth/token', auth=('oauth-key', 'oauth-secret'), data=post_data, json={'grant_type=client_credentials'})\n",
    "\n",
    "data = requestOauth.json();\n",
    "print(data['access_token'])\n",
    "access_token = data['access_token']\n",
    "\n",
    "headers = {'Content-type': 'application/json', 'Authorization': 'Bearer {}'.format(access_token)}\n",
    "#Read the test dataframe and stream each row\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "#for index, row in features_test.toPandas().iterrows():\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    str1 = ','.join(str(e) for e in  data)\n",
    "    requestPrediction = requests.post('<INSERT SELDON API SERVER URL>/api/v0.1/predictions', headers=headers, json={\"strData\": str1 })\n",
    "    predictionData = requestPrediction.json();\n",
    "    datafield = predictionData['data']\n",
    "    predictionArray = datafield['ndarray']\n",
    "    print(predictionArray[0])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project frauddetection\n",
    "oc delete seldondeployments mymodel\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
