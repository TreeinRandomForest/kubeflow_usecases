{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version:\n",
    "* Reads data from a CSV file using pandas (Spark to come later)\n",
    "* Does no preprocessing (where Spark should really help for large datasets\n",
    "* Doesn't do anything about class imbalance except to look at recall and precision\n",
    "* Trains both a sklearn (random forest) and a neural network (PyTorch) model\n",
    "* Is not how a data scientist at a bank would build a statistically robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labels: Extreme class imbalance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Distribution of labels: Extreme class imbalance')\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a4cade7b8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Time', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    227437\n",
       "1       408\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56962, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    56878\n",
       "1       84\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "df_test['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "\n",
    "model.fit(df_train.drop('Class', axis=1), df_train['Class'])\n",
    "\n",
    "pred_train_prob = model.predict_proba(df_train.drop('Class', axis=1))\n",
    "pred_test_prob = model.predict_proba(df_test.drop('Class', axis=1))\n",
    "\n",
    "pred_train = model.predict(df_train.drop('Class', axis=1))\n",
    "pred_test = model.predict(df_test.drop('Class', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(df, pred):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    unique_labels = df['Class'].unique()\n",
    "    N = len(unique_labels)\n",
    "    confusion = confusion_matrix(df['Class'], pred)\n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(N))\n",
    "    ax.set_yticks(range(N))\n",
    "\n",
    "    ax.set_xticklabels(unique_labels)\n",
    "    ax.set_yticklabels(unique_labels)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Labels')\n",
    "\n",
    "plot_confusion_matrix(df_train, pred_train);\n",
    "plot_confusion_matrix(df_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_precision_recall_curve(df, pred_prob, label):\n",
    "    precision, recall, thresholds = precision_recall_curve(df['Class'], pred_prob[:,1])\n",
    "    plt.plot(precision, recall, 'p-', label=label)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Precision vs Recall Trade-off Curve')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "plot_precision_recall_curve(df_train, pred_train_prob, 'Train')\n",
    "plot_precision_recall_curve(df_test, pred_test_prob, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PyTorch Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_hidden = 128\n",
    "N_output = 1\n",
    "\n",
    "net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(N_hidden, N_output),\n",
    "                    #nn.Sigmoid()\n",
    "                   )\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "        \n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "ds_torch_train = CCDataset(df_train)\n",
    "ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "dl_torch_train = DataLoader(ds_torch_train, batch_size=128, num_workers=0)\n",
    "dl_torch_test = DataLoader(ds_torch_test, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.3660, -0.1107,  0.3921, -2.4874,  0.6832, -0.5036,  0.0537,  0.4164,\n",
      "        -2.0171, -0.3664,  0.3445, -0.2090,  0.4674,  0.2977, -0.9051,  1.5516,\n",
      "        -0.3594, -0.4669,  0.3072,  0.2416,  0.5749,  1.0821, -0.5869,  0.3493,\n",
      "         1.1521,  0.0832, -0.1562, -0.1348, 34.9000]), tensor([0.], dtype=torch.float64))\n",
      "V1        -1.366025\n",
      "V2        -0.110730\n",
      "V3         0.392079\n",
      "V4        -2.487366\n",
      "V5         0.683165\n",
      "V6        -0.503618\n",
      "V7         0.053736\n",
      "V8         0.416353\n",
      "V9        -2.017139\n",
      "V10       -0.366418\n",
      "V11        0.344452\n",
      "V12       -0.208980\n",
      "V13        0.467398\n",
      "V14        0.297699\n",
      "V15       -0.905122\n",
      "V16        1.551645\n",
      "V17       -0.359367\n",
      "V18       -0.466922\n",
      "V19        0.307226\n",
      "V20        0.241633\n",
      "V21        0.574859\n",
      "V22        1.082113\n",
      "V23       -0.586899\n",
      "V24        0.349253\n",
      "V25        1.152088\n",
      "V26        0.083172\n",
      "V27       -0.156159\n",
      "V28       -0.134787\n",
      "Amount    34.900000\n",
      "Class      0.000000\n",
      "Name: 217021, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ds_torch_train[0])\n",
    "print(df_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter = 0 Train Loss = 5.748235945747854e-05 val_loss = 3.99165837734472e-05 val_avg_precision = 0.7865594108348966\n",
      "Iter = 1 Train Loss = 6.82578239517132e-05 val_loss = 0.00012758166121784598 val_avg_precision = 0.7053932949822509\n",
      "Iter = 2 Train Loss = 4.811245652574875e-05 val_loss = 3.182401633239351e-05 val_avg_precision = 0.8174455980977119\n",
      "Iter = 3 Train Loss = 5.083140621134799e-05 val_loss = 2.471364859957248e-05 val_avg_precision = 0.8143604177633129\n",
      "Iter = 4 Train Loss = 3.979746548633679e-05 val_loss = 2.7066984330303967e-05 val_avg_precision = 0.8143491355049851\n",
      "Iter = 5 Train Loss = 3.271951227659751e-05 val_loss = 2.4657736503286287e-05 val_avg_precision = 0.8252187563235743\n",
      "Iter = 6 Train Loss = 3.636503236096434e-05 val_loss = 2.5971932700485922e-05 val_avg_precision = 0.8258509206126553\n",
      "Iter = 7 Train Loss = 2.96559877258913e-05 val_loss = 2.703062455111649e-05 val_avg_precision = 0.813902132832901\n",
      "Iter = 8 Train Loss = 4.010179360764968e-05 val_loss = 2.4004944862099364e-05 val_avg_precision = 0.8183229824084297\n",
      "Iter = 9 Train Loss = 3.0811732397678224e-05 val_loss = 2.5204439225490205e-05 val_avg_precision = 0.8353842836091191\n"
     ]
    }
   ],
   "source": [
    "model, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, 10, 1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO DEBUG THIS\n",
    "#criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1., 1000.]))\n",
    "#model, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion_weighted, 10, 1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Script\n",
    "\n",
    "For many tools, hyperparameter tuning using Katib for example, we will need a script that can accept command line arguments and package it in an image. Since many data scientists prefer using Jupyter notebooks, we'll add this section to write a main function and convert it to a script. An alternative is to do your development work in scripts and import the scripts in a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single node script\n",
    "\n",
    "The cell below writes the contents to a file called ccfraud.py. It doesn't execute the contents in the cell.\n",
    "\n",
    "The training is done on a single physical node with the only parallelization possible being one across threads on the processor on the node and on an attached GPU.\n",
    "\n",
    "For parallelization by splitting the data (but not the model) across multiple physical nodes, please see the next example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccfraud.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ccfraud.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_architecture(df_train, N_hidden=128):\n",
    "    '''Wrapper around the neural net architecture\n",
    "    '''\n",
    "    N_output = 1\n",
    "\n",
    "    net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(N_hidden, N_output),\n",
    "                        #nn.Sigmoid()\n",
    "                       )\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "\n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = get_architecture(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple nodes script\n",
    "\n",
    "The cell below writes the contents to a file called ccfraud_distributed.py. It doesn't execute the contents in the cell.\n",
    "\n",
    "The training is done across multiple physical nodes. torch.distributed has a range of options for distributing across both a single physical node with multiple GPUs as well as across multiple physical nodes either by having each node compute gradients on a fraction of the data or even by having subsets of the model across different nodes.\n",
    "\n",
    "Example: https://github.com/kubeflow/pytorch-operator/blob/master/examples/mnist/mnist.py\n",
    "\n",
    "torch.distributed: start here https://pytorch.org/tutorials/beginner/dist_overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccfraud_distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ccfraud_distributed.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))\n",
    "print(f'WORLD_SIZE = {WORLD_SIZE}')\n",
    "\n",
    "def get_architecture(df_train, N_hidden=128):\n",
    "    '''Wrapper around the neural net architecture\n",
    "    '''\n",
    "    N_output = 1\n",
    "\n",
    "    net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(N_hidden, N_output),\n",
    "                        #nn.Sigmoid()\n",
    "                       )\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1\n",
    "    \n",
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()\n",
    "    \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "        \n",
    "    #distributed\n",
    "    if should_distribute():\n",
    "        dist.init_process_group(dist.Backend.GLOO)\n",
    "        print('Using distributed PyTorch with backend GLOO')\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = get_architecture(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    if is_distributed():\n",
    "        Distributor = nn.parallel.DistributedDataParallelCPU\n",
    "        net = Distributor(net)\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Single Node Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccfraud_serving.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ccfraud_serving.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, df_train, N_hidden=128):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.N_input = df_train.shape[1]-1\n",
    "        self.N_output = 1\n",
    "        \n",
    "        self.layer1 = nn.Linear(self.N_input, N_hidden)\n",
    "        self.layer2 = nn.Linear(N_hidden, self.N_output)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.act(self.layer1(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "\n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = Net(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "\n",
    "    torch.save(net.state_dict(), \"model.pt\")\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "feat_imp = sorted(zip(features_train_pd.columns, model.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in feat_imp], [i[1] for i in feat_imp], 'p-')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target variables for convenience.\n",
    "## From the graph we only want seven important features V3,V4,V10,V11,V12,V14,V17\n",
    "drop_time_class = ['_c0', 'Time', 'Class','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "drop_class=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "features_test.printSchema()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=6, n_jobs=10, class_weight='balanced')\n",
    "                               \n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))\n",
    "  \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_ = plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = ['V3','V4','V10','V11','V12','V14','V17','Amount'])\n",
    "    print(model.predict(rowdf))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Rook/Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"model.pkl\")\n",
    "prefix='uploaded/'\n",
    "result = s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenShift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o oc.tar.gz -L https://mirror.openshift.com/pub/openshift-v3/clients/4.0.22/linux/oc.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp oc ~/../bin/oc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login into Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc login -u <INSERT USERNAME> -p <INSERT PASSWORD> --insecure-skip-tls-verify <INSERT CLUSTER URL>:6443\n",
    "oc project frauddetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model With Seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project frauddetection\n",
    "oc create -n frauddetection -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/mymodel.json\n",
    "oc get seldondeployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model in Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp jq ~/../bin/jq\n",
    "chmod 777 ~/../bin/jq\n",
    "export TOKENJSON=$(curl -XPOST -u oauth-key:oauth-secret <INSERT SELDON API SERVER URL>/oauth/token -d 'grant_type=client_credentials')\n",
    "export TOKEN=$(echo $TOKENJSON | jq \".access_token\" -r)\n",
    "echo $TOKEN\n",
    "\n",
    "curl -v --header \"Authorization: Bearer $TOKEN\" <INSERT SELDON API SERVER URL>/api/v0.1/predictions -d '{\"strData\": \"0.365194527642578,0.819750231339882,-0.5927999453145171,-0.619484351930421,-2.84752569239798,1.48432160780265,0.499518887687186,72.98\"}' -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the served model from python using the test dataframe\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Get the token\n",
    "post_data = {\"grant_type\": \"client_credentials\"}\n",
    "requestOauth = requests.post('<INSERT SELDON API SERVER URL>/oauth/token', auth=('oauth-key', 'oauth-secret'), data=post_data, json={'grant_type=client_credentials'})\n",
    "\n",
    "data = requestOauth.json();\n",
    "print(data['access_token'])\n",
    "access_token = data['access_token']\n",
    "\n",
    "headers = {'Content-type': 'application/json', 'Authorization': 'Bearer {}'.format(access_token)}\n",
    "#Read the test dataframe and stream each row\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "#for index, row in features_test.toPandas().iterrows():\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    str1 = ','.join(str(e) for e in  data)\n",
    "    requestPrediction = requests.post('<INSERT SELDON API SERVER URL>/api/v0.1/predictions', headers=headers, json={\"strData\": str1 })\n",
    "    predictionData = requestPrediction.json();\n",
    "    datafield = predictionData['data']\n",
    "    predictionArray = datafield['ndarray']\n",
    "    print(predictionArray[0])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project frauddetection\n",
    "oc delete seldondeployments mymodel\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
